* Wed 18 Mar 18:39:45 GMT 2015
** Pound
Relayd isn't working well, so we need a replacement.
Suggested/recommended Pound.

    pkg_add pound

Now, let's empty the config file.

#+BEGIN_SRC 
-bash-4.3# > /etc/pound.cfg
-bash-4.3# pound
starting...
no listeners defined - aborted
#+END_SRC

Great, so we need some listeners.  They can be HTTP or HTTPS.  Let's
start with an HTTP one.  Listeners need an address and a port.  The
address is just localhost, and the port can be 8080.

This is enough for pound to start:

#+BEGIN_SRC 
# telnet localhost 8080
Trying 127.0.0.1...
Connected to localhost.
Escape character is '^]'.
get / http/1.1

HTTP/1.0 503 Service Unavailable
Content-Type: text/html
Content-Length: 53
Expires: now
Pragma: no-cache
Cache-control: no-cache,no-store

The service is not available. Please try again later.Connection closed by foreign host.
#+END_SRC

Fairly obviously, pound doesn't know what to do with requests.  Let's
fix that.  For this we need a service.  Services can either be global
or tied to listeners.  For now let's make it global.

Now, services need backends.  Again, the simplest thing possible would
be to pass everything to one machine.

This works:

#+BEGIN_SRC 
-bash-4.3# telnet localhost 8080
Trying 127.0.0.1...
Connected to localhost.
Escape character is '^]'.
get / http/1.1

HTTP/1.1 400 Bad Request
Date: Wed, 18 Mar 2015 19:11:26 GMT
Server: Apache/2.4.7 (Ubuntu)
Content-Length: 320
Connection: close
Content-Type: text/html; charset=iso-8859-1

<!DOCTYPE HTML PUBLIC "-//IETF//DTD HTML 2.0//EN">
<html><head>
<title>400 Bad Request</title>
</head><body>
<h1>Bad Request</h1>
<p>Your browser sent a request that this server could not understand.<br />
</p>
<hr>
<address>Apache/2.4.7 (Ubuntu) Server at l0001.rs.uk.livelinkprint.com Port 80</address>
</body></html>
Connection closed by foreign host.
#+END_SRC


Now, let's make it a bit more complicated.

Let's have two services, one for costcophoto.co.uk, and one for
lift.livelinkprint.com.  OK that works... (be careful with spaces in
vhost names - the space needs to be there to match Host:
myhost.foo.com)

OK, next, can we have multiple matches per service, so we can pass in
for, eg, s28 and s29 domains?  No.  We need a service per host.

Right, this all seems to work.  The next challenge is an HTTPS listener.

So, for this we need to add a listener and cert.

The cert needs all three in one:

    -bash-4.3# cat livelinkprint.com.key livelinkprint.com.crt livelinkprint.com.crt2 > livelinkprint.com.pem

So with the HTTPS listener in place, everything works and is in Chef
and the service is running.  But we need it to be in chef.

Not all straightfoward in Chef as the package and service resources
don't quite do the right thing, so I've botched it using execute
resources.

OK, in Chef and pushed.
* Thu 19 Mar 09:12:20 GMT 2015
** SNS Log
Maintaining a log is a good idea.  But let's ensure it's in git, so I can get at it any time:

#+BEGIN_SRC
sns@bosch:~/doc$ cd sns/
sns@bosch:~/doc/sns$ ls
sns.org
sns@bosch:~/doc/sns$ git init
Initialized empty Git repository in /home/sns/doc/sns/.git/
sns@bosch:~/doc/sns$ git add sns.org 
sns@bosch:~/doc/sns$ git commit -m 'SNS Log'
[master (root-commit) 0611a8d] SNS Log
 1 file changed, 134 insertions(+)
 create mode 100644 sns.org
sns@bosch:~/doc/sns$ git remote add origin git@github.com:Atalanta/snslog.git
sns@bosch:~/doc/sns$ git push -u origin master
Counting objects: 3, done.
Delta compression using up to 4 threads.
Compressing objects: 100% (2/2), done.
Writing objects: 100% (3/3), 2.08 KiB | 0 bytes/s, done.
Total 3 (delta 0), reused 0 (delta 0)
To git@github.com:Atalanta/snslog.git
 * [new branch]      master -> master
Branch master set up to track remote branch master from origin.
#+END_SRC
** Emacs Learnings
- To prevent ido-mode from switching to a file/buffer that is already open (such as default.rb), use *ido-fix* (C-j)
- To create src or quote blocks quickly, in org-mode, type in "<s" or "<q" and press TAB
** Pound follow up
Pound has been installed on the main firewall:

#+BEGIN_SRC
Host rsbastion
  HostName 109.200.18.70
  User sns
  Port 3023
IdentityFile ~/.ssh/id_dsa
#+END_SRC

Chef has been updated accordingly, and relayd switched off.  The
firewall is now configured to redirect http and https traffic to
localhost, wherefrom Pound sends it to the right places:

#+BEGIN_SRC
pass in quick on $ext_if proto tcp from any to $ext_if port http rdr-to 127.0.0.1
pass in quick on $ext_if proto tcp from any to $ext_if port https rdr-to 127.0.0.1
#+END_SRC

Pound is running out of rc.d, but not monitored or supervised in
anyway.  It would be a good idea to have the process watched using
monit.

Logs presently are in pound format, but there's an option to make them
apache format which might be useful for analysis purposes.

The logs are probably not being rotated, and there is no other monitoring on the box.

- Created ticket:
  https://livelinkinfraops.zendesk.wcom/agent/tickets/130 for log
  rotation.

- Created ticket:
  https://livelinkinfraops.zendesk.com/agent/tickets/131 for
  monitoring.

Use of Relayd also isn't documented, which is largely a function of
the wiki not being available.

The process supervision is required for both pound and thin, so it's
important to get right.
** Wiki Unavailable
Discussed with NT & SB.  Current status is that the wiki exists in so
far as there's a git repo, but it is hard to use.  I'd already created
a prototype wiki at 37.153.107.245.

This used varnish to proxy to thin, and managed thin using monit:

#+BEGIN_SRC
[root@28e8a2a4-7db4-ea0c-afeb-be0b963334f0 ~]# cat /opt/local/etc/default.vcl 
backend default {
    .host = "127.0.0.1";
    .port = "3000";
}

sub vcl_fetch {
    if (beresp.status == 302 && beresp.http.location ~ "/create") { return (hit_for_pass); }  }
#+END_SRC

This is needed because when you create a site, there's a redirect, which gets cached.  Actually there are other problems with Varnish which are unresolved:

#+BEGIN_SRC
09:14 <Cope> Still have an oddity with my varnish / gollum setup... 
09:15 <Cope> http://37.153.107.245:8080/Home
09:15 <Cope> 1) Create a page & save
09:15 <Cope> 2) Edit the page and save
09:15 <Cope> When we visit the edit page we don't (always) see the latest version
09:15 <Cope> and if we do
09:16 <Cope> 3) When we edit it again, we see a cache of the last edit!
09:16 <Cope> turns out caching is hard.
09:37 <Mithrandir> either have it send out a purge on edit, or look at what it looks like when you edit, then purge the object for that url
09:41 <Cope> what's the best way to see what is actually happening?
09:41 <Mithrandir> I tend to just use the network viewer in chromium
09:41 <Mithrandir> but wireshark works too
#+END_SRC

To run with thin, we just need a config.ru in the base of the directory we want to use.  Nick has augmented this to implement some authentication:

#+BEGIN_SRC 
__DIR__ = File.expand_path(File.dirname(__FILE__))
$: << __DIR__
require 'rubygems'
require 'yaml'
require 'app'
App.set(:gollum_path, __DIR__)
App.set(:authorized_users, YAML.load_file(File.expand_path('users.yml', __DIR__)))
App.set(:wiki_options, {})
run App 
#+END_SRC

This requires that we have an app.rb and a users.yml file too:

#+BEGIN_SRC 
require 'gollum/app'
require 'digest/sha1'

class App < Precious::App
  User = Struct.new(:name, :email, :password_hash, :can_write)

  before { authenticate! }
  before /^\/(edit|create|delete|livepreview|revert)/ do authorize_write! ; end

  helpers do
    def authenticate!
      @_auth ||=  Rack::Auth::Basic::Request.new(request.env)
      if @_auth.provided?
      end
      if @_auth.provided? && @_auth.basic? && @_auth.credentials &&
        @user = detected_user(@_auth.credentials)
        return @user
      else
        response['WWW-Authenticate'] = %(Basic realm="Gollum Wiki")
        throw(:halt, [401, "Not authorized\n"])
      end
    end

    def authorize_write!
      throw(:halt, [403, "Forbidden\n"]) unless @user.can_write
    end

    def users
      @_users ||= settings.authorized_users.map {|u| User.new(*u) }
    end

    def detected_user(credentials)
      users.detect do |u|
        [u.email, u.password_hash] ==
        [credentials[0], Digest::SHA1.hexdigest(credentials[1])]
      end
    end
  end

  def commit_message
    {
      :message => params[:message],
      :name => @user.name,
      :email => @user.email
    }
  end
end
#+END_SRC

The users.yml looks like this:

#+BEGIN_SRC 
---
- - Nick Trew
  - n.trew@livelinktechnology.net
  - e5e9fa1ba31ecd1ae84f75caaa474f3a663f05f4 # secret
  - true 
#+END_SRC

So NT & SB and I talked about the hosting environment for the wiki.  I
suggested continuing with thin, but with apache and mod_proxy_balancer
in place of passenger.  SB pointed out that we do wish to keep things
standard, but was open to trying an alternative, as long as we agreed
to standardise.

The requirement for authentication was also discussed.  I suggested
using Basic Auth with Apache, but NT managed to find a way to do it in
the application.  Either way, it works fine.  We just need to ensure
we run it via https (using the wildcard certificate).

We agreed that we'd like to run this in RedStation.

Next steps:

- Finish the monit supervision to run with multiple thin processes
- Write an SMF manifest to run monit
- Get Chef to drop off the user.yml

Nick's making a wiki cookbook.
** Chef Environment Issue
Yesterday I made breaking changes to the firewall cookbook (replacing
relayd with pound), so bumped the version to 1.0.0.  Using knife
spork, I did cookbook version bumping and promoting, which worked
fine.  The state being that when I had finished, the testing
environment specified 1.0.5, the firewall was using the testing
environment, and the newest version (with the pound changes) was
uploaded.

This morning, SJH made changes to correct the egress and ingress rules
for UDP and TCP, and duly bumped the version and environment.
However, because I failed to push my environment to git, Stuart's
local copy was from a time when the version was 0.12ish, so he ended
up setting the environment to a lower version.  This could have had
the effect of pushing out changes from an earlier version.

Lesson: ensure you push your environments!

Thoughts from Jon:

#+BEGIN_SRC 
13:35 <Cope> https://gist.github.com/Atalanta/5b067ad755a2e79acd6d
13:36 <jonlives> the short version for how we catch that is https://github.com/bmarini/knife-inspect, run as a jenkins job on every git push
13:41 <Cope> so jenkins watches git
13:41 <Cope> and runs knife-inspect
13:41 <Cope> and bitches out
13:42 <jonlives> if anything is out of sync
13:42 <jonlives> which includes constraints etc
13:42 <Cope> so it won't stop the push, but it will warn us that it is wrong
13:42 <jonlives> yeah
#+END_SRC
** Test Kitchen for OpenBSD and SmartOS
NT has implemented some of this... need to find out how much.
Specifically I am thinking of the fact that the firewall is using the
testing environment, but that the testing environment lags behind the
latest version.
** FahyFoto R3 Site
https://livelinkinfraops.zendesk.com/agent/tickets/124
Not sure what the process is here, or what the deadline is.

Damon says there's no real deadline, but that things should really
speed up a bit.  The dude has been waiting for 2 weeks.  I'm not sure
what the status us, but I've said I'll look into it.  Also I've
suggested that we have a chat with Ed and map out the flow and see
what we can do to speed it up.



** Walmart Hardware Order
Yesterday I had a long session with Guy in which we outlined our
thoughts about size and spec of machines.  I need to convert this into
a spreadsheet and write it up for David today.  In principle, I think
we are of the view that 60 drives machines could be very effective.
To that end I asked Sentral to match the basic spec that Doug and
Robert at Hammer had come up with.  They've done so, and come in at
about the price I expected.

I also had a very long conversation with the founder/ceo of 'Scalable
Informatics', whom Heinz recommended.  He also knows Phil Hollenback.
They build 60 drives machines to a high spec, and layout the ZFS stuff
and offer a 3 year support package.  This seems excellent but looks
like coming in at about 2 or 3 times more expensive per box, which at
11 boxes is not insignificant.  I think perhaps we might want to try
out one of them?
** Provisioning a Ruby Site
Spoke with SJH & SB about the steps to set up a Ruby site, such that
it's ready to be deployed with Capistrano.  SJH outlined the following:

- Provision Apache & Passenger
- Create a deploy user
- Add a vhost
- Create an SMF manifest for apache with secret.key.base
- Create a DB (if it doesn't exist)
- Grant the privileges / create the user
- Render the shared/database.yml
- Create an internal DNS record for the zone
- Create an external DNS record for the zone
- Add appropriate entries to the pound config

I gave a history lesson about starting from raw resources, then using
definitions, then LWRPs then resources in a library, and recommended
they start with the newest approach.  SJH asked about orchestration,
and pointed out that Chef is very much focussed on a node by node
basis, but that he is thinking at a higher level - how would we do this is Chef?

I mentioned Chef Provisioning as something to look at, but also felt
that for much of the stuff simply using Chef search ought to be
sufficient.
