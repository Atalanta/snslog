* Sun Mar 15 12:50:05 GMT 2015
** Things on my mind:

- Get the wiki up
- IRC cat
- Monitoring live
- Base on SmartOS
- ACL working
- S28/29 spreadsheet

Soas usual I am feeling like there's a big chain of dependencies.  I'd
like to do some stuff on SmartOS.  I'd like to be able to test it.  I
could do that manually by creating machines in JPC, and managing them
manually.  That would get things done.  It would be better to get that
plugged into TK though.

I'm going to go with primitive.

** Base SmartOS

This should be pretty straightforward.  First create a box.  Go to
JPC.  Click on create an instance, select base-64-lts, pick the size.

So next, what's the simplest, fastest way to get this machine
provisioned?  Again, it'd be great to have a knife bootstrap template.
But for now I'll do it manually.

OK so we have the IP:  72.2.113.95.  My Bosch key is good for LLJPC.

Now, for the purposes of an erb template, we need:

#+BEGIN_SRC 
pkgin -y in build-essential
pkgin -y in ruby
pkgin -y in ruby215-readline
gem install chef --no-rdoc --no-ri
#+END_SRC

I wonder if there is a better SmartOS omnibus package.  Seems not
really.  So we can look into this.

So now that chef is installed, we just need to keys and stuff.  I have
these on bosch.

Getting to bosch from here is easy enough because I configured
ssh... but which key?

#+BEGIN_SRC 
vermeer$ ssh-agent bash
vermeer$ ssh-add /home/sns/.ssh/id_dsa (Heidegger)
vermeer$ ssh-add /home/sns/.ssh/sns-laptop (Gentlemen)
vermeer$ ssh-add /home/sns/.ssh/bosch (None) **** THIS ONE
#+END_SRC

From Bosch:

#+BEGIN_SRC 
sns@bosch:/tmp$ rsync -Pvar chef root@72.2.113.95:/etc
#+END_SRC

And then on wikitest:

#+BEGIN_SRC 
# chef-client
DL is deprecated, please use Fiddle
[2015-03-15T13:38:42+00:00] INFO: Forking chef instance to converge...
Starting Chef Client, version 12.1.1
[2015-03-15T13:38:43+00:00] INFO: *** Chef 12.1.1 ***
[2015-03-15T13:38:43+00:00] INFO: Chef-client pid: 32168
Creating a new client identity for wikitest1 using the validator key.
[2015-03-15T13:38:46+00:00] INFO: Client key /etc/chef/client.pem is not present - registering
[2015-03-15T13:38:47+00:00] INFO: HTTP Request Returned 404 Object Not Found: error
[2015-03-15T13:38:48+00:00] INFO: Run List is []
[2015-03-15T13:38:48+00:00] INFO: Run List expands to []
[2015-03-15T13:38:48+00:00] INFO: Starting Chef Run for wikitest1
[2015-03-15T13:38:48+00:00] INFO: Running start handlers
[2015-03-15T13:38:48+00:00] INFO: Start handlers complete.
resolving cookbooks for run list: []
[2015-03-15T13:38:48+00:00] INFO: Loading cookbooks []
Synchronizing Cookbooks:
Compiling Cookbooks...
[2015-03-15T13:38:48+00:00] WARN: Node wikitest1 has an empty run list.
Converging 0 resources
[2015-03-15T13:38:48+00:00] INFO: Chef Run complete in 0.789150147 seconds

Running handlers:
[2015-03-15T13:38:48+00:00] INFO: Running report handlers
Running handlers complete
[2015-03-15T13:38:48+00:00] INFO: Report handlers complete
Chef Client finished, 0/0 resources updated in 5.862599973 seconds
[2015-03-15T13:38:48+00:00] INFO: Sending resource update report (run-id: ffc699d3-b25b-431a-80e1-d4a599c3c7d6)
#+END_SRC

OK... so we want to check out the base:

#+BEGIN_SRC
sns@bosch:~/src/livelink-chef-repo/roles$ cat base.rb 
name 'base'
description 'Shared role for all systems'
run_list 'recipe[base]'
#+END_SRC

Update the runlist:

#+BEGIN_SRC 
sns@bosch:~/src/livelink-chef-repo$ knife node run_list set wikitest1 'role[base]'
#+END_SRC


Running chef-client--- bugger:

#+BEGIN_SRC 
    * cannot determine group id for 'sbradly', does the group exist on this system?
#+END_SRC


OK, so, on SmartOS the user resource won't create a group.  Feels to
me like the acl cookbook is needed now.

** ACL Cookbook
#+BEGIN_SRC 
20:00 <Cope> I have a requirement to manage users on a fairly granular level, in terms of what each user can (and cannot) do, and over what machines.
20:01 <Cope> my idea is to have an acls object (currently from a databag) which has keys corresponding to hostnames or patterns to match hostnames, and a default
20:02 <Cope> with the values corresponding to a scale of permission level, where -1 is locked, 0 is no user, 1 is restricted user, 2 is login user, 3 is login user with some sudo power and 4 is full sudo
20:02 <Cope> so each user gets one of these objects
20:02 <Cope> and then on each node a decision can be made as to what sort of shell, what sort of sudo privs, etc should be in place
20:03 <Cope> i wanted to avoud a whole mess of conditional logic in recipe code
20:03 <Cope> so something that generates the data which defines the appropriate level of access for a user on a node
20:03 <Cope> which is then handed to regular resources
20:04 <Cope> this needs to work on disparate platforms too, so again, generating the data first, and working out what needs to be called could be done outside of teh recipe
20:05 <Cope> i notice that the cool kids don't lwrp any more... so that's a consideratiob
20:05 <Cope> anyway
20:05 <Cope> thats the outline idea... what do you think?
#+END_SRC

Looked at MySQL cookbook.  Seems like this is a good pattern.  Parking for now.
** LeoFS Call with Heinz Geis
Scale - few big servers
600TB in each (not ideal)
Would prefer more small servers
Works better this way
recovery is better
rep set to 3
Why fewer large?  because there before
may as well use them
3TB SATAs x 60

Packages in project fifo
Operationally: hard to monitor
Checks if machines are down
JSON calls to API
Behaves well
if it goes down (system crash, network failure) restart
been using since 0.6
No problems since 1.1
Entire network rewiring
Update procedure - a bit manual
Tell machine down, update and back up again
Cannot chaneg replication factor after
Need to decide now
3 storage zones - then add a 2nd... and
very easy to add
say where is the master
no masterless setup
ring like (dynamo)
2 management servers (client/master)
coordination here
rebalance suffles
1 gateway
zfs compression on storage servers
still benefit (over many images)
jpeg header always same... so there is some benefit
zfs is smart - won't copress
what goes wrong?
worst - compact?
manually tell it to delete old data
not perfect with s3 clients
large datasets - no problem with 
large data in chyunks
1GB zone image --> leofs chunk into 5MB chunks
smaller just stored as 1 object
can send zfs data into leofs
chunk reads & writes
uploads chunks in parallel
download is the same
tricky to sync r/w size
classical approach of streaming up and down, gts around problem
1-2MB no problem
s3cmd or dragondisk osx - classic read and write non-chunked
part of s3 api (so can ignore it... which would be fine)

LeoFS - multi data centre aware
async bkg replication between datacentres
very sensible & smart
keep 1 extra copy in each remote DC *(or more)
3 copies + 1 in remote
for DR... if we die, we have the data
and quicker remote read access
Japanese Amazon!
Share data
Perf baseline:

cat sat 10G ether
multi writes in parallel
reads: 5GB +/-
stripe of mirrors
stripe of raidz2
mirrors zfs log & cache
scalable informatics
1/2 TB memory
e5 3.4 procesors
512GB

2U many disks (20ish)
128+ memory
reads - memory... good
fifo contains monitoring
rest by zabbix
ZFS se#nd is 100MB/s
concat witg gzip -1
sweet spot...
50GB....

data frequently accessed in l1 and l2 arc

most data not needed; can build who storage to be fast
can be as big as memory

data on disk - lowest
ssd act as l2 arc (not in memory... often used)
l 1 = zfs using spare memory

cache = 2 x ssds

4TB ssd cache
many disks

zpool

1GB memory & 100GB disk - lose performance
becasue where stuff is layed out on disk is in memory... is there a rule of thumb

l1 -> l2 -> arc cache

zil / slog? enough to written to logs

picked because R&W perf is much better
basho sales were dicks
* Wed 18 Mar 18:39:45 GMT 2015
** Pound
Relayd isn't working well, so we need a replacement.
Suggested/recommended Pound.

    pkg_add pound

Now, let's empty the config file.

#+BEGIN_SRC 
-bash-4.3# > /etc/pound.cfg
-bash-4.3# pound
starting...
no listeners defined - aborted
#+END_SRC

Great, so we need some listeners.  They can be HTTP or HTTPS.  Let's
start with an HTTP one.  Listeners need an address and a port.  The
address is just localhost, and the port can be 8080.

This is enough for pound to start:

#+BEGIN_SRC 
# telnet localhost 8080
Trying 127.0.0.1...
Connected to localhost.
Escape character is '^]'.
get / http/1.1

HTTP/1.0 503 Service Unavailable
Content-Type: text/html
Content-Length: 53
Expires: now
Pragma: no-cache
Cache-control: no-cache,no-store

The service is not available. Please try again later.Connection closed by foreign host.
#+END_SRC

Fairly obviously, pound doesn't know what to do with requests.  Let's
fix that.  For this we need a service.  Services can either be global
or tied to listeners.  For now let's make it global.

Now, services need backends.  Again, the simplest thing possible would
be to pass everything to one machine.

This works:

#+BEGIN_SRC 
-bash-4.3# telnet localhost 8080
Trying 127.0.0.1...
Connected to localhost.
Escape character is '^]'.
get / http/1.1

HTTP/1.1 400 Bad Request
Date: Wed, 18 Mar 2015 19:11:26 GMT
Server: Apache/2.4.7 (Ubuntu)
Content-Length: 320
Connection: close
Content-Type: text/html; charset=iso-8859-1

<!DOCTYPE HTML PUBLIC "-//IETF//DTD HTML 2.0//EN">
<html><head>
<title>400 Bad Request</title>
</head><body>
<h1>Bad Request</h1>
<p>Your browser sent a request that this server could not understand.<br />
</p>
<hr>
<address>Apache/2.4.7 (Ubuntu) Server at l0001.rs.uk.livelinkprint.com Port 80</address>
</body></html>
Connection closed by foreign host.
#+END_SRC


Now, let's make it a bit more complicated.

Let's have two services, one for costcophoto.co.uk, and one for
lift.livelinkprint.com.  OK that works... (be careful with spaces in
vhost names - the space needs to be there to match Host:
myhost.foo.com)

OK, next, can we have multiple matches per service, so we can pass in
for, eg, s28 and s29 domains?  No.  We need a service per host.

Right, this all seems to work.  The next challenge is an HTTPS listener.

So, for this we need to add a listener and cert.

The cert needs all three in one:

    -bash-4.3# cat livelinkprint.com.key livelinkprint.com.crt livelinkprint.com.crt2 > livelinkprint.com.pem

So with the HTTPS listener in place, everything works and is in Chef
and the service is running.  But we need it to be in chef.

Not all straightfoward in Chef as the package and service resources
don't quite do the right thing, so I've botched it using execute
resources.

OK, in Chef and pushed.
* Thu 19 Mar 09:12:20 GMT 2015
** SNS Log
Maintaining a log is a good idea.  But let's ensure it's in git, so I can get at it any time:

#+BEGIN_SRC
sns@bosch:~/doc$ cd sns/
sns@bosch:~/doc/sns$ ls
sns.org
sns@bosch:~/doc/sns$ git init
Initialized empty Git repository in /home/sns/doc/sns/.git/
sns@bosch:~/doc/sns$ git add sns.org 
sns@bosch:~/doc/sns$ git commit -m 'SNS Log'
[master (root-commit) 0611a8d] SNS Log
 1 file changed, 134 insertions(+)
 create mode 100644 sns.org
sns@bosch:~/doc/sns$ git remote add origin git@github.com:Atalanta/snslog.git
sns@bosch:~/doc/sns$ git push -u origin master
Counting objects: 3, done.
Delta compression using up to 4 threads.
Compressing objects: 100% (2/2), done.
Writing objects: 100% (3/3), 2.08 KiB | 0 bytes/s, done.
Total 3 (delta 0), reused 0 (delta 0)
To git@github.com:Atalanta/snslog.git
 * [new branch]      master -> master
Branch master set up to track remote branch master from origin.
#+END_SRC
** Emacs Learnings
- To prevent ido-mode from switching to a file/buffer that is already open (such as default.rb), use *ido-fix* (C-j)
- To create src or quote blocks quickly, in org-mode, type in "<s" or "<q" and press TAB
** Pound follow up
Pound has been installed on the main firewall:

#+BEGIN_SRC
Host rsbastion
  HostName 109.200.18.70
  User sns
  Port 3023
IdentityFile ~/.ssh/id_dsa
#+END_SRC

Chef has been updated accordingly, and relayd switched off.  The
firewall is now configured to redirect http and https traffic to
localhost, wherefrom Pound sends it to the right places:

#+BEGIN_SRC
pass in quick on $ext_if proto tcp from any to $ext_if port http rdr-to 127.0.0.1
pass in quick on $ext_if proto tcp from any to $ext_if port https rdr-to 127.0.0.1
#+END_SRC

Pound is running out of rc.d, but not monitored or supervised in
anyway.  It would be a good idea to have the process watched using
monit.

Logs presently are in pound format, but there's an option to make them
apache format which might be useful for analysis purposes.

The logs are probably not being rotated, and there is no other monitoring on the box.

- Created ticket:
  https://livelinkinfraops.zendesk.wcom/agent/tickets/130 for log
  rotation.

- Created ticket:
  https://livelinkinfraops.zendesk.com/agent/tickets/131 for
  monitoring.

Use of Relayd also isn't documented, which is largely a function of
the wiki not being available.

The process supervision is required for both pound and thin, so it's
important to get right.
** Wiki Unavailable
Discussed with NT & SB.  Current status is that the wiki exists in so
far as there's a git repo, but it is hard to use.  I'd already created
a prototype wiki at 37.153.107.245.

This used varnish to proxy to thin, and managed thin using monit:

#+BEGIN_SRC
[root@28e8a2a4-7db4-ea0c-afeb-be0b963334f0 ~]# cat /opt/local/etc/default.vcl 
backend default {
    .host = "127.0.0.1";
    .port = "3000";
}

sub vcl_fetch {
    if (beresp.status == 302 && beresp.http.location ~ "/create") { return (hit_for_pass); }  }
#+END_SRC

This is needed because when you create a site, there's a redirect, which gets cached.  Actually there are other problems with Varnish which are unresolved:

#+BEGIN_SRC
09:14 <Cope> Still have an oddity with my varnish / gollum setup... 
09:15 <Cope> http://37.153.107.245:8080/Home
09:15 <Cope> 1) Create a page & save
09:15 <Cope> 2) Edit the page and save
09:15 <Cope> When we visit the edit page we don't (always) see the latest version
09:15 <Cope> and if we do
09:16 <Cope> 3) When we edit it again, we see a cache of the last edit!
09:16 <Cope> turns out caching is hard.
09:37 <Mithrandir> either have it send out a purge on edit, or look at what it looks like when you edit, then purge the object for that url
09:41 <Cope> what's the best way to see what is actually happening?
09:41 <Mithrandir> I tend to just use the network viewer in chromium
09:41 <Mithrandir> but wireshark works too
#+END_SRC

To run with thin, we just need a config.ru in the base of the directory we want to use.  Nick has augmented this to implement some authentication:

#+BEGIN_SRC 
__DIR__ = File.expand_path(File.dirname(__FILE__))
$: << __DIR__
require 'rubygems'
require 'yaml'
require 'app'
App.set(:gollum_path, __DIR__)
App.set(:authorized_users, YAML.load_file(File.expand_path('users.yml', __DIR__)))
App.set(:wiki_options, {})
run App 
#+END_SRC

This requires that we have an app.rb and a users.yml file too:

#+BEGIN_SRC 
require 'gollum/app'
require 'digest/sha1'

class App < Precious::App
  User = Struct.new(:name, :email, :password_hash, :can_write)

  before { authenticate! }
  before /^\/(edit|create|delete|livepreview|revert)/ do authorize_write! ; end

  helpers do
    def authenticate!
      @_auth ||=  Rack::Auth::Basic::Request.new(request.env)
      if @_auth.provided?
      end
      if @_auth.provided? && @_auth.basic? && @_auth.credentials &&
        @user = detected_user(@_auth.credentials)
        return @user
      else
        response['WWW-Authenticate'] = %(Basic realm="Gollum Wiki")
        throw(:halt, [401, "Not authorized\n"])
      end
    end

    def authorize_write!
      throw(:halt, [403, "Forbidden\n"]) unless @user.can_write
    end

    def users
      @_users ||= settings.authorized_users.map {|u| User.new(*u) }
    end

    def detected_user(credentials)
      users.detect do |u|
        [u.email, u.password_hash] ==
        [credentials[0], Digest::SHA1.hexdigest(credentials[1])]
      end
    end
  end

  def commit_message
    {
      :message => params[:message],
      :name => @user.name,
      :email => @user.email
    }
  end
end
#+END_SRC

The users.yml looks like this:

#+BEGIN_SRC 
---
- - Nick Trew
  - n.trew@livelinktechnology.net
  - e5e9fa1ba31ecd1ae84f75caaa474f3a663f05f4 # secret
  - true 
#+END_SRC

So NT & SB and I talked about the hosting environment for the wiki.  I
suggested continuing with thin, but with apache and mod_proxy_balancer
in place of passenger.  SB pointed out that we do wish to keep things
standard, but was open to trying an alternative, as long as we agreed
to standardise.

The requirement for authentication was also discussed.  I suggested
using Basic Auth with Apache, but NT managed to find a way to do it in
the application.  Either way, it works fine.  We just need to ensure
we run it via https (using the wildcard certificate).

We agreed that we'd like to run this in RedStation.

Next steps:

- Finish the monit supervision to run with multiple thin processes
- Write an SMF manifest to run monit
- Get Chef to drop off the user.yml

Nick's making a wiki cookbook.
** Chef Environment Issue
Yesterday I made breaking changes to the firewall cookbook (replacing
relayd with pound), so bumped the version to 1.0.0.  Using knife
spork, I did cookbook version bumping and promoting, which worked
fine.  The state being that when I had finished, the testing
environment specified 1.0.5, the firewall was using the testing
environment, and the newest version (with the pound changes) was
uploaded.

This morning, SJH made changes to correct the egress and ingress rules
for UDP and TCP, and duly bumped the version and environment.
However, because I failed to push my environment to git, Stuart's
local copy was from a time when the version was 0.12ish, so he ended
up setting the environment to a lower version.  This could have had
the effect of pushing out changes from an earlier version.

Lesson: ensure you push your environments!

Thoughts from Jon:

#+BEGIN_SRC 
13:35 <Cope> https://gist.github.com/Atalanta/5b067ad755a2e79acd6d
13:36 <jonlives> the short version for how we catch that is https://github.com/bmarini/knife-inspect, run as a jenkins job on every git push
13:41 <Cope> so jenkins watches git
13:41 <Cope> and runs knife-inspect
13:41 <Cope> and bitches out
13:42 <jonlives> if anything is out of sync
13:42 <jonlives> which includes constraints etc
13:42 <Cope> so it won't stop the push, but it will warn us that it is wrong
13:42 <jonlives> yeah
#+END_SRC
** Monit Process Supervision
#+BEGIN_SRC 
[root@28e8a2a4-7db4-ea0c-afeb-be0b963334f0 /opt/local/etc/monit]# grep -v ^# monitrc 
set daemon  60              # check services at 1-minute intervals
set mailserver localhost
set alert sanelson@gmail.com not on { instance, action }
set httpd port 2812 and
    allow admin:monit      # require user 'admin' with password 'monit'
    allow @monit           # allow users of group 'monit' to connect (rw)
    allow @users readonly  # allow users of group 'users' to connect readonly


check process thin with pidfile /root/mithrandir/tmp/pids/thin.pid
    start program = "/opt/local/bin/thin -d -c /root/mithrandir start"
    stop program = "/opt/local/bin/thin -c /root/mithrandir stop"
#+END_SRC

So monit needs to know:

- what the process should be called
- what the pid is
- what the start program is
- what the stop program is

For thin, we'd want to have separate processes for each one, rather
than start a cluster, as deriving the pids would be trickier.

Looking at the help, a few things catch my eye:

-  -S, --socket FILE                bind to unix domain socket
-  -P, --pid FILE                   File to store PID (default: tmp/pids/thin.pid)
-  -s, --servers NUM                Number of servers to start

Actually it seems like the best approach is to put the various options into a config.yml file, like this:

#+BEGIN_SRC 
# cat config.yml
---
user: nobody
group: nobody
pid: /var/run/thin.pid
timeout: 30
wait: 30
log: log/thin.log
max_conns: 1024
require: []
environment: production
max_persistent_conns: 512
servers: 4
threaded: true
daemonize: true
socket: tmp/sockets/thin.sock
chdir: /root/mithrandir
#+END_SRC

This allows the thin processes to be controlled using the -o, --only
option, so we can start and stop a cluster one by one.

Monit can do this with stanzas like this:

#+BEGIN_SRC 
check process thin-0 
    with pidfile /var/run/thin.0.pid
    start program = "/opt/local/bin/thin -C /root/mithrandir/config.yml -o 0 start"
    stop program  = "/opt/local/bin/thin -C /root/mithrandir/config.yml -o 0 stop"
#+END_SRC

We can run a bunch of them like this:

#+BEGIN_SRC 
check process thin-0 
    with pidfile /var/run/thin.0.pid
    start program = "/opt/local/bin/thin -C /root/mithrandir/config.yml -o 0 start"
    stop program  = "/opt/local/bin/thin -C /root/mithrandir/config.yml -o 0 stop"

check process thin-1
    with pidfile /var/run/thin.1.pid
    start program = "/opt/local/bin/thin -C /root/mithrandir/config.yml -o 1 start"
    stop program  = "/opt/local/bin/thin -C /root/mithrandir/config.yml -o 1 stop"

check process thin-2
    with pidfile /var/run/thin.2.pid
    start program = "/opt/local/bin/thin -C /root/mithrandir/config.yml -o 2 start"
    stop program  = "/opt/local/bin/thin -C /root/mithrandir/config.yml -o 2 stop"

check process thin-3 
    with pidfile /var/run/thin.3.pid
    start program = "/opt/local/bin/thin -C /root/mithrandir/config.yml -o 3 start"
    stop program  = "/opt/local/bin/thin -C /root/mithrandir/config.yml -o 3 stop"
#+END_SRC

Obviously we need to put the app somewhere sensible, but that's detail.
** Apache 2.4 and Unix Domain Sockets
Apache 2.4 (since 2.4.7) can talk to Unix Domain Sockets in mod_proxy:

#+BEGIN_SRC 
<VirtualHost *:80>
        ServerName infraopswiki.livelinkprint.com
        DocumentRoot /root/mithrandir

        <Proxy balancer://mycluster/>
                BalancerMember unix:/root/mithrandir/tmp/sockets/thin.0.sock|http://localhost
                BalancerMember unix:/root/mithrandir/tmp/sockets/thin.1.sock|http://localhost
                BalancerMember unix:/root/mithrandir/tmp/sockets/thin.2.sock|http://localhost
                BalancerMember unix:/root/mithrandir/tmp/sockets/thin.3.sock|http://localhost
        </Proxy>
        ProxyPass / balancer://mycluster/       
</VirtualHost>

#+END_SRC
** Push after or before a promote --remote
This is a discussion - we need to do one or the other.  I know Etsy
have struggled with this.  They settled for after, because the server
is the source of truth for the nodes, and git for the humans.

** Test Kitchen for OpenBSD and SmartOS
NT has implemented some of this... need to find out how much.
Specifically I am thinking of the fact that the firewall is using the
testing environment, but that the testing environment lags behind the
latest version.
** FahyFoto R3 Site
https://livelinkinfraops.zendesk.com/agent/tickets/124
Not sure what the process is here, or what the deadline is.

Damon says there's no real deadline, but that things should really
speed up a bit.  The dude has been waiting for 2 weeks.  I'm not sure
what the status us, but I've said I'll look into it.  Also I've
suggested that we have a chat with Ed and map out the flow and see
what we can do to speed it up.



** Walmart Hardware Order
Yesterday I had a long session with Guy in which we outlined our
thoughts about size and spec of machines.  I need to convert this into
a spreadsheet and write it up for David today.  In principle, I think
we are of the view that 60 drives machines could be very effective.
To that end I asked Sentral to match the basic spec that Doug and
Robert at Hammer had come up with.  They've done so, and come in at
about the price I expected.

I also had a very long conversation with the founder/ceo of 'Scalable
Informatics', whom Heinz recommended.  He also knows Phil Hollenback.
They build 60 drives machines to a high spec, and layout the ZFS stuff
and offer a 3 year support package.  This seems excellent but looks
like coming in at about 2 or 3 times more expensive per box, which at
11 boxes is not insignificant.  I think perhaps we might want to try
out one of them?
** Provisioning a Ruby Site
Spoke with SJH & SB about the steps to set up a Ruby site, such that
it's ready to be deployed with Capistrano.  SJH outlined the following:

- Provision Apache & Passenger
- Create a deploy user
- Add a vhost
- Create an SMF manifest for apache with secret.key.base
- Create a DB (if it doesn't exist)
- Grant the privileges / create the user
- Render the shared/database.yml
- Create an internal DNS record for the zone
- Create an external DNS record for the zone
- Add appropriate entries to the pound config

I gave a history lesson about starting from raw resources, then using
definitions, then LWRPs then resources in a library, and recommended
they start with the newest approach.  SJH asked about orchestration,
and pointed out that Chef is very much focussed on a node by node
basis, but that he is thinking at a higher level - how would we do this is Chef?

I mentioned Chef Provisioning as something to look at, but also felt
that for much of the stuff simply using Chef search ought to be
sufficient.
** CyrusOne Call
Spoke with Colleen this morning (her time).  She's working to get the
paperwork ready into a signable bundle by tomorrow morning her time.
From our side we simply need to decide if we're happy with the MSA and
standard T&C.  I think we are, but I need to check with David.

On my side, we need to agree the size of network we need, so we can
justify IP addresses.

Touch call again tomorrow at the same time.
** Sentral Storage Discussion
Had a call with Mike & Mike.  They say that they cannot get the 60
drive machine until late April, which is less than ideal.  They
suggest a 1U machine with the same spec and one or two 4U JBODs.
These have:

- 45 3.5 drives per chassis in a 4U enclosure
- 4 ports out the back

They also have one which takes 90, but this is 2 drives per hotswap,
which would require some zpool thinking.

The Mikes are going to get back to me with pricing ASAP.
** Ganglia
#+BEGIN_SRC 
marking ganglia-webfrontend-3.1.2nb2 as non auto-removable
[root@737e21f2-8f99-e8f9-ff4e-8a530796050e ~]# lynx http://localhost/ganglia/
[root@737e21f2-8f99-e8f9-ff4e-8a530796050e ~]# ifconfig 
lo0: flags=2001000849<UP,LOOPBACK,RUNNING,MULTICAST,IPv4,VIRTUAL> mtu 8232 index 1
        inet 127.0.0.1 netmask ff000000 
net0: flags=40201000843<UP,BROADCAST,RUNNING,MULTICAST,IPv4,CoS,L3PROTECT> mtu 1500 index 2
        inet 10.128.2.6 netmask ffffff00 broadcast 10.128.2.255
        ether 90:b8:d0:e2:ee:e2 
lo0: flags=2002000849<UP,LOOPBACK,RUNNING,MULTICAST,IPv6,VIRTUAL> mtu 8252 index 1
        inet6 ::1/128 
[root@737e21f2-8f99-e8f9-ff4e-8a530796050e ~]# man svccfg
[root@737e21f2-8f99-e8f9-ff4e-8a530796050e ~]# logout
Connection to 10.128.2.6 closed.
#+END_SRC
* Sat Mar 21 20:31:48 UTC 2015
** Workstation
I managed to fill up / on my OpenBSD workstation, so I needed a
rebuild.  Decided to give FreeBSD a go.  Downloaded the image and
discovered that on the mac it is very slow to dd, resulting in me
being impatient, and pulling out the stick before it was finished.  To
show progress as dd runs, we can use pv:

#+BEGIN_SRC 
pv -ptearb FreeBSD-10.1-RELEASE-amd64-mini-memstick.img | dd of=/dev/rdisk2 bs=1024
#+END_SRC

I installed FreeBSD and then went into /usr/ports/x11/xorg and did a make install clean.

I forgot to do a make config-recursive, but remembered next time!

Next up: i3lock, i3status and i3.

Other things I will want:

- screen
- emacs
- firefox
- ruby
- dmenu
- rxvt-unicode
- feh?
- inconsolata font

Once all installed, I rsync'd back from Chenrezig.  This is all a bit
messy, and needs to be tidied up, but I have all teh data, and the
Firefox cache even came over!

One consideration is that FreeBSD does not read .Xresources by
default. And when I copied it over to .Xdefaults, it came up in pink!

However, xrdb ~/.Xresources worked fine, so I shall add that to my .xinitrc, which currently says:

#+BEGIN_SRC 
[[ -f ~/.Xresources ]] && xrdb -merge ~/.Xresources
setxkbmap -layout gb -option ctrl:nocaps &
xsetroot -solid black &
exec i3
#+END_SRC

So I think if I just remove the -merge bit, it should be fine.  Let's test that out.

OK, this doesn't work... i need to do a manual xrdb from an xterm after.  That's something to iron out later.

Added git, and built ruby from src using ruby-install, and managed with chruby.
** Loadbalancer and Wiki
So the thinking before was that the firewall (loadbalancer) was going
to be done properly, ie documented well.  So we started thinking about
the wiki.  We now have a working wiki prototype, with apache, and
authentication.  That means in theory we could get it up and running
on JPC, and then move it to RS later.  I'd rather do it in RS
properly, TBH.

*** Nick's Tests
First, let's look at nick's wiki cookbook.  OK awesome - it has tests.
Let's try running them!

OK, first need to install Test Kitchen:

#+BEGIN_SRC 
>>>>>> Message: Could not load the 'joyent' driver from the load path. Please ensure that your driver is installed as a gem or included in your Gemfile if using Bundler.
#+END_SRC

OK, also need kitchen-joyent... and Berkshelf.

Aha this falls over on gecode:
*** Gecode Issues on FreeBSD

#+BEGIN_SRC 
/usr/home/sns/.gem/ruby/2.2.1/gems/dep-selector-libgecode-1.0.2/ext/libgecode3/vendor/gecode-3.7.3/configure: 11561: Syntax error: word unexpected (expecting ")")
extconf.rb:98:in `block in run': Failed to build gecode library. (GecodeBuild::BuildError)
        from extconf.rb:97:in `chdir'
        from extconf.rb:97:in `run'
        from extconf.rb:104:in `<main>'

extconf failed, exit code 1
#+END_SRC

There's an issue for this: https://github.com/berkshelf/berkshelf/issues/1280

The recommendation is to use system gecode: USE_SYSTEM_GECODE=1 gem install berkshelf

However, doing this requires that we use ~>3.5 gecode.  Ports currently provides 4.3.

https://secure.freshbsd.org/search?project=freebsd-ports&q=gecode

Now, if we install portdowngrade, we can run:

portdowngrade devel/gecode r345033

So this installs gecode, which should mean we can now set USE_SYSTEM_GECODE.  However, this didn't find it:

https://gist.github.com/Atalanta/1e3fcfaa53436731adad

I think this is because if I used FreeBSD's Ruby, the system path
would be set, but since I just installed it using ruby-install
locally, it doesn't know where to look.

Noah pointed me to using Bundler instead:

#+BEGIN_SRC 
 bundle config build.dep_selector "--with-cflags=\"-I$HOME/gecode-3.7.3/include\"
--with-cppflags=\"-I$HOME/gecode-3.7.3/include\" --with-ldflags=\"-L$HOME/gecode-3.7.3/lib
-Wl,-rpath=$HOME/gecode-3.7.3/lib\""
#+END_SRC

But this still fails:

#+BEGIN_SRC 
Gem::Ext::BuildError: ERROR: Failed to build gem native extension.

    /home/sns/.rubies/ruby-2.2.1/bin/ruby -r ./siteconf20150321-98277-1wukayp.rb extconf.rb --with-cflags='-I ./gecode-3.7.3/include' --with-cppflags='-I ./gecode-3.7.3/include' --with-ldflags='-L ./gecode-3.7.3/lib -Wl,-rpath=./gecode-3.7.3/lib'
checking for main() in -lgecodesupport... no
checking for main() in -lgecodesupport... no
#+END_SRC

Going to try just doing it manually.  I already know I need to change
the shebang in the configure script to #!/usr/bin/env bash.

#+BEGIN_SRC 
./configure
gmake
gmake install
#+END_SRC

#+BEGIN_SRC 
USE_SYSTEM_GECODE=1 gem install berkshelf --no-ri --no-rdoc -- -with-cflags="-I/usr/local/include" --with-cppflags="-I/usr/local/include" --with-ldflags="-L/usr/local/lib -Wl,-rpath=/usr/local/lib"
#+END_SRC

This works.  The bundler invocation was referring to an Ubuntu binary.

Back to Kitchen.
*** Kitchen Joyent
#+BEGIN_SRC 
$ kitchen create   
-----> Starting Kitchen (v1.3.1)
-----> Creating <default-smartos-1430>...
>>>>>> ------Exception-------
>>>>>> Class: Kitchen::ActionFailed
>>>>>> Message: Failed to complete #create action: [options[:joyent_keyfile] provided does not exist.]
>>>>>> ----------------------
>>>>>> Please see .kitchen/logs/kitchen.log for more details
>>>>>> Also try running `kitchen diagnose --all` for configuration
#+END_SRC

Aha, that's because I am using Nick's .kitchen.yml:

#+BEGIN_SRC 
$ cat .kitchen.yml 
---
driver:
  name: joyent
  joyent_username: testkitchen
  joyent_keyfile: /home/nick/.ssh/id_rsa
  joyent_keyname: 74:cc:45:6b:00:5a:a5:0d:45:29:8c:5c:ba:9c:39:42
  joyent_url: https://10.129.128.4
  joyent_ssl_verify_peer: false

provisioner:
  name: chef_zero

platforms:
  - name: smartos-14.3.0
    driver:
      joyent_image_id: 62f148f8-6e84-11e4-82c5-efca60348b9f
      joyent_flavor_id: 'Chef Basic'
    provisioner:
      chef_omnibus_url: https://gist.githubusercontent.com/vxnick/f6e56edba66798638156/raw
      chef_client_path: /opt/local/bin/chef-client
suites:
  - name: default
    run_list:
      - recipe[wiki::default]
    attributes:
      wiki:
        testing: true
    data_bags_path: "test/integration/data_bags"
#+END_SRC

So a few things occur to me here:

- I don't have Nick's Key
- I don't have access to SDC from home
- The flavor is set to 'Chef Basic' which is obviously something custom which won't be in JPC

The key isn't an issue.  My own key is on SDC.  I do need the
fingerprint, but that'll be in the SDC interface.  So as long as I
have access to SDC, I should be fine.  So, let's go to Bosch.  OK from
here I can see SDC, but I need the SDC address internally, and a proxy.

MCH SDC is at:

- https://10.129.128.2/ (20150302 - SDC)

Which I ought to have access to from mch-bastion.  So, let's set up a tunnel.

#+BEGIN_SRC 
ssh -D 8080 mch-bastion
#+END_SRC

Now I can hit SDC, and login (admin/Earl205Roberts).

OK, so a user/key - Added bosch.pub:

- 97:12:64:8a:34:8e:a8:46:fe:48:a6:d5:85:ca:93:c2

I should be able to shut my proxy off now, and change my .kitchen.yml, and then run the tests.

Added teh following to .kitchen.local.yml

#+BEGIN_SRC 
---
driver:
  name: joyent
  joyent_username: testkitchen
  joyent_keyfile: /home/sns/.ssh/id_dsa
  joyent_keyname: 97:12:64:8a:34:8e:a8:46:fe:48:a6:d5:85:ca:93:c2
  joyent_url: https://10.129.128.4
  joyent_ssl_verify_peer: false
#+END_SRC


$ kitchen create
-----> Starting Kitchen (v1.3.1)
-----> Creating <default-smartos-1430>...
       Joyent <92a6de15-81db-e96e-c36f-d290011255d2> created.
................................       (server ready)
       (ssh ready)

       Finished creating <default-smartos-1430> (0m33.77s).
-----> Kitchen is finished. (0m34.28s)

$ kitchen destroy
-----> Starting Kitchen (v1.3.1)
-----> Destroying <default-smartos-1430>...
       Joyent instance <92a6de15-81db-e96e-c36f-d290011255d2> destroyed.
       Finished destroying <default-smartos-1430> (0m0.47s).
-----> Kitchen is finished. (0m0.99s)

OK, let's try a converge:

#+BEGIN_SRC 
downloading https://gist.githubusercontent.com/vxnick/f6e56edba66798638156/raw to file /tmp/install.sh
#+END_SRC

OK this is slow, installing all this stuff.  This should go in an
image, and we should invest time in getting a SmartOS omnibus package.

Still it works.
*** Finishing the cookbook
So Nick's cookbook won't converge properly:

#+BEGIN_SRC 
 ---- Begin output of thin -C /opt/local/wiki/thin.yml start ----
       STDOUT: 
       STDERR: /opt/local/lib/ruby/gems/2.1.4/gems/thin-1.6.3/lib/thin/daemonizing.rb:171:in `remove_stale_pid_file': tmp/pids/thin.pid already exists, seems like it's already running (process ID: 8197). Stop the process or delete tmp/pids/thin.pid. (Thin::PidFileExist)
#+END_SRC

OK, so we want to get this up and running.  We have the basic stuff
installed, and we can easily get the apache and thin config in place.
What it doesn't have is monit, which we use to supervise the service.

There's of course a who watches the watcher, so let's get monit running in SMF.

#+BEGIN_SRC 
pkgin install py27-pip
pip install manifold
manifold monit.xml
svccfg import monit.xml 
svcadm enable monit
#+END_SRC

This works just fine.  So for our recipe, we could drop the file off
somewhere, and import it with an execute resource, and enable it.

Better would be to use Eric's SMF LWRP, which I'll try next.
** X Fiddling
Had a play with i3lock, and got it to match Solarized colours:

#+BEGIN_SRC 
i3lock -c 002b36
#+END_SRC

And a play with i3status, to get it to return a bit more data:

#+BEGIN_SRC 
$ cat .i3config 
 general {
                   output_format = "term"
                   colors = true
                   interval = 5
           }

           order += "disk /"
           order += "ethernet em0"
           order += "load"
           order += "tztime local"

           ethernet em0 {
                   # if you use %speed, i3status requires the cap_net_admin capability
                   format_up = "E: %ip (%speed)"
                   format_down = "E: down"
           }

           tztime local {
                   format = "%Y-%m-%d %H:%M:%S"
           }

           load {
                   format = "%5min"
           }

           disk "/" {
                   format = "%free"
           }
#+END_SRC

* Sun Mar 22 08:43:32 GMT 2015
** Monit / SMF
We're at the stage with the wiki that it's close to working, and we
have a cookbook which can be tested, and which runs on SmartOS.  The
main missing feature was the process supervision of Thin.  This could
be handled with SMF, but given that the plan is to run monit on all
machines, for alerting, we may as well use it to supervise (and
monitor/alert) Thin too.

We reached the point where we have a working monit config for the
thins, and needed to install and start (and monitor) monit itself.
SMF is the perfect candidate for this.  I built a simple manifest
using Manifold which seems to do the trick, but then I found the SMF
LWRP which looks like a potentially better approach.

My manifest looks like this:

#+BEGIN_SRC 
<?xml version="1.0"?>
<!DOCTYPE service_bundle SYSTEM "/usr/share/lib/xml/dtd/service_bundle.dtd.1">
<!--
        Created by Manifold
--><service_bundle type="manifest" name="monit">

    <service name="site/monit" type="service" version="1">

        <create_default_instance enabled="false"/>
        
        <single_instance/>

        <dependency name="network" grouping="require_all" restart_on="error" type="service">
            <service_fmri value="svc:/milestone/network:default"/>
        </dependency>

        <dependency name="filesystem" grouping="require_all" restart_on="error" type="service">
            <service_fmri value="svc:/system/filesystem/local"/>
        </dependency>


        
        
        
        
        <method_context>
            <method_credential user="root" group="root"/>
        </method_context>

        <exec_method type="method" name="start" exec="/opt/local/bin/monit" timeout_seconds="60"/>

        <exec_method type="method" name="stop" exec=":kill" timeout_seconds="60"/>

        <property_group name="startd" type="framework">
            
            
            <propval name="duration" type="astring" value="contract"/>
            <propval name="ignore_error" type="astring" value="core,signal"/>
        </property_group>

        <property_group name="application" type="application">
            <propval name="config_file" type="astring" value="/opt/local/etc/monit/monitrc"/>
        </property_group>
        
        
        <stability value="Evolving"/>

        <template>
            <common_name>
                <loctext xml:lang="C">
                    Monit
                </loctext>
            </common_name>
        </template>

    </service>

</service_bundle>
#+END_SRC

Nick's tests fail because the node won't converge, as there's no guard
on the execute.  I've removed the  execute, and am converging again to
see if the  earlier tests pass.  Well, the node  converges, which is a
start.  Excellent - those tests pass.
** Getting Base Tests Passing
- Copied my.kitchen.local.yml over, and the .kitchen.yml from wiki
- examining diff
#+BEGIN_SRC 
$ git diff .kitchen.yml
diff --git a/cookbooks/base/.kitchen.yml b/cookbooks/base/.kitchen.yml
index 47d832e..6ee485d 100644
--- a/cookbooks/base/.kitchen.yml
+++ b/cookbooks/base/.kitchen.yml
@@ -1,10 +1,7 @@
 ---
 driver:
   name: joyent
-  # vagrantfile_erb: VagrantOverride.erb
   joyent_username: testkitchen
-  joyent_keyfile: /home/nick/.ssh/id_rsa
-  joyent_keyname: 74:cc:45:6b:00:5a:a5:0d:45:29:8c:5c:ba:9c:39:42
   joyent_url: https://10.129.128.4
   joyent_ssl_verify_peer: false
 
@@ -17,33 +14,13 @@ platforms:
       joyent_image_id: 62f148f8-6e84-11e4-82c5-efca60348b9f
       joyent_flavor_id: 'Chef Basic'
     provisioner:
-      # require_chef_omnibus: true
-      chef_omnibus_url: https://gist.githubusercontent.com/vxnick/f6e56edba66798638156/raw/cbc5cc25a87c192044438bcef3833ad2c05a5e3c/gistfile1.txt
+      chef_omnibus_url: https://gist.githubusercontent.com/vxnick/f6e56edba66798638156/raw
       chef_client_path: /opt/local/bin/chef-client
-  - name: ubuntu-14.04
-    driver:
-      joyent_image_id: b7493690-f019-4612-958b-bab5f844283e
-      joyent_flavor_id: 'Chef Basic'
-
-  # - name: ubuntu-14.04
-  #   driver:
-  #     box: opscode-ubuntu-14.04
-  # - name: openbsd-5.6
-  #   provisioner:
-  #     require_chef_omnibus: false
-  #   driver:
-  #     box: openbsd-5.6
-  #   busser:
-  #     root_path: /opt/busser
-
 suites:
   - name: default
     run_list:
-      - recipe[base::default]
+      - recipe[wiki::default]
     attributes:
-      chef_client:
-        depends_client_rb: false
-      openssh:
-        server:
-          port: [22, 3023]
+      wiki:
+        testing: true
     data_bags_path: "test/integration/data_bags"
#+END_SRC

Right, merged that, removed commented code.  Testing a create/converge
for base.  Create works as expected... let's see what happens with a converge.

OK this fails as expected, because the users manage LWRP doesn't work
on SmartOS.  We're going to replace this with the ACL cookbook
shortly, but for now, given that SDC allows us access to the machine,
we can install the rest of the base stuff.

Not sure why the dd-agent-postfix thing is in the smartos cookbook - I
think this is actually another case where if we did this correcty
(with the ACLs cookbook), it wouldn't be needed.

#+BEGIN_SRC 
           Chef::Exceptions::EnclosingDirectoryDoesNotExist
           ------------------------------------------------
           Parent directory /etc/sudoers.d does not exist.
#+END_SRC

This should be handled by attributes:

#+BEGIN_SRC 
default['authorization']['sudo']['include_sudoers_d'] = true
#+END_SRC


So we need to debug this.

Installed pry (using pkgin, so we don't get readline nasties).  Now Chef runs:

#+BEGIN_SRC 
/opt/local/bin/chef-client --local-mode --config /tmp/kitchen/client.rb --log_level debug --force-formatter --no-color --chef-zero-port 8889 --json-attributes /tmp/kitchen/dna.json
#+END_SRC


OK, so we see that the attr is set:

#+BEGIN_SRC 
"authorization"=>
    {"sudo"=>
      {"groups"=>["wheel", "infraops", "sudo"],
       "users"=>[],
       "passwordless"=>true,
       "include_sudoers_d"=>true,
       "agent_forwarding"=>false,
       "sudoers_defaults"=>["!lecture,tty_tickets,!fqdn"],
       "command_aliases"=>[],
       "prefix"=>"/opt/local/etc"}}
#+END_SRC

Aha:

#+BEGIN_SRC 
Recipe: sudo::default
  * smartos_package[sudo] action install (skipped due to not_if)
  * directory[/opt/local/etc/sudoers.d] action create (up to date)
  * cookbook_file[/opt/local/etc/sudoers.d/README] action create (up to date)
  * template[/opt/local/etc/sudoers] action create (up to date)
#+END_SRC

Right, fixing the sudoers package makes base converge.  Minus the users of course.
Let's run the tests: aha
** Fixing Busser
#+BEGIN_SRC 
$ kitchen verify smart
-----> Starting Kitchen (v1.3.1)
-----> Setting up <default-smartos-1430>...
       sh: line 4: /opt/chef/embedded/bin/ruby: not found
       sudo: /opt/chef/embedded/bin/gem: command not found
       sudo: /opt/chef/embedded/bin/gem: command not found
       sudo: /busser: command not found
       sudo: /tmp/busser/bin/busser: command not found
>>>>>> Setup failed on instance <default-smartos-1430>.
>>>>>> Please see .kitchen/logs/default-smartos-1430.log for more details
>>>>>> ------Exception-------
>>>>>> Class: Kitchen::ActionFailed
>>>>>> Message: SSH exited (1) for command: [sh -c '
BUSSER_ROOT="/tmp/busser" GEM_HOME="/tmp/busser/gems" GEM_PATH="/tmp/busser/gems" GEM_CACHE="/tmp/busser/gems/cache" 
export BUSSER_ROOT GEM_HOME GEM_PATH GEM_CACHE
gem_bindir=`/opt/chef/embedded/bin/ruby -rrubygems -e "puts Gem.bindir"`

if ! sudo -E /opt/chef/embedded/bin/gem list busser -i >/dev/null; then
  sudo -E /opt/chef/embedded/bin/gem install busser --no-rdoc --no-ri
fi
sudo -E ${gem_bindir}/busser setup
sudo -E /tmp/busser/bin/busser plugin install busser-serverspec
']
>>>>>> ----------------------
#+END_SRC

#+BEGIN_SRC 
D      Setup failed on instance <default-smartos-1430>.
D      ------Exception-------
D      Class: Kitchen::InstanceFailure
D      Message: Setup failed on instance <default-smartos-1430>.  Please see .kitchen/logs/default-smartos-1430.log for more details
D      ---Nested Exception---
D      Class: Kitchen::ActionFailed
D      Message: SSH exited (1) for command: [sh -c '
BUSSER_ROOT="/tmp/busser" GEM_HOME="/tmp/busser/gems" GEM_PATH="/tmp/busser/gems" GEM_CACHE="/tmp/busser/gems/cache" 
export BUSSER_ROOT GEM_HOME GEM_PATH GEM_CACHE
gem_bindir=`/opt/chef/embedded/bin/ruby -rrubygems -e "puts Gem.bindir"`

if ! sudo -E /opt/chef/embedded/bin/gem list busser -i >/dev/null; then
  sudo -E /opt/chef/embedded/bin/gem install busser --no-rdoc --no-ri
fi
sudo -E ${gem_bindir}/busser setup
#+END_SRC

#+BEGIN_SRC 
[sns@vermeer ~/src/test-kitchen]$ grep gem_bindir lib/kitchen/busser.rb 
        gem_bindir=`#{ruby} -rrubygems -e "puts Gem.bindir"`
        #{sudo("${gem_bindir}")}/busser setup
#+END_SRC


Ruby comes from:

#+BEGIN_SRC 
      ruby    = "#{config[:ruby_bindir]}/ruby"
#+END_SRC


So we need to set ruby_bindir

#+BEGIN_SRC 
      @config[:ruby_bindir] = opts.fetch(:ruby_bindir, DEFAULT_RUBY_BINDIR)
#+END_SRC


So we need to get ruby_bindir into the config hash.
This comes from data.busser_data_for in config.rb

This uses merged_data_for... which I think must come from a suite

OK... so basically we have a busser entry in .kitchen.yml

#+BEGIN_SRC 
 busser:
      busser_bin: "/tmp/busser/bin/busser"
      kitchen_root: "/home/sns/src/livelink-chef-repo/cookbooks/base"
      root_path: "/tmp/busser"
      ruby_bindir: "/opt/chef/embedded/bin"
      sudo: true
      suite_name: default
      test_base_path: "/home/sns/src/livelink-chef-repo/cookbooks/base/test/integration"
      version: busser
#+END_SRC

So we can insert this under a platform, and, lo, it works!
** Adding Monit Tests
OK, so in base we want to install and start Monit with a default config.  There's a package everywhere.

#+BEGIN_SRC 
       Failures:
        
         1) base::default runs the monit web service
            Failure/Error: expect(port(2812)).to be_listening
              expected Port "2812" to be listening
              /bin/sh -c netstat\ -tunl\ \|\ grep\ --\ :2812\\\ 
              /native/usr/bin/netstat: illegal option -- t
       usage: /native/usr/bin/netstat [-anv] [-f address_family] [-T d|u]
       /native/usr/bin/netstat [-n] [-f address_family] [-P protocol] [-T d|u] [-g | -p | -s [interval [count]]]
       /native/usr/bin/netstat -m [-v] [-T d|u] [interval [count]]
       /native/usr/bin/netstat -i [-I interface] [-an] [-f address_family] [-T d|u] [interval [count]]
       /native/usr/bin/netstat -r [-anv] [-f address_family|filter] [-T d|u]
       /native/usr/bin/netstat -M [-ns] [-f address_family] [-T d|u]
       /native/usr/bin/netstat -D [-I interface] [-f address_family] [-T d|u]

#+END_SRC

We're using a branded zone here... which is obviously using native
netstat.  We'll just use netstat -an to geta round this.

#+BEGIN_SRC 
 base::default
         installs tmux (if needed)
         installs vim
         installs monit
         starts the monit daemon
         runs the monit web service
       
       Finished in 1.41 seconds (files took 0.29756 seconds to load)
       5 examples, 0 failures
       
       Finished verifying <default-ubuntu-1404> (0m16.21s).
-----> Destroying <default-ubuntu-1404>...
       Joyent instance <7f59e9df-3af3-4c39-caf5-f447917cf4a5> destroyed.
       Finished destroying <default-ubuntu-1404> (0m0.96s).
       Finished testing <default-ubuntu-1404> (3m18.83s).
-----> Kitchen is finished. (3m19.33s)
#+END_SRC

Getting smartos working was as simple as using the SMF cookbook.  All passing.
** Using Monit in Wiki
OK, so I'm thinking that monit should really be its own cookbook,
because otherwise, in order to use monit, we need to include base,
which doesn't feel right.  However, just supposing we did:

#+BEGIN_SRC 
include_recipe 'base'
#+END_SRC


OK - now we have monit.

So we could then:

- Drop off a thin file in monit.d
- Send monit a reload

Which is ok for now.

First things first, let's run these tests again:

#+BEGIN_SRC 
       * cannot determine user id for 'wiki', does the user exist on this system?
           ================================================================================
           Error executing action `create` on resource 'template[/opt/local/wiki/thin.yml]'
           ================================================================================
           
           Chef::Exceptions::UserIDNotFound
           --------------------------------
           cannot determine user id for 'wiki', does the user exist on this system?
           
           Resource Declaration:
           ---------------------
           # In /tmp/kitchen/cache/cookbooks/wiki/recipes/default.rb
       
            34: template "#{wiki_root_path}/thin.yml" do
            35:   source 'thin-yml.erb'
        36:   variables(wiki_root_path: wiki_root_path)
            37:   owner 'wiki'
        38:   group 'wiki'
        39: end
            40: 

#+END_SRC



Obviously I accidentally trashed some stuff!  So I've put that back,
and it converges.  Adding the base stuff in, though, results in an SMF
dependency issue.  Looks like Nick made a change to the chef-client cookbook
to make test-kitchen work.

#+BEGIN_SRC 
    attributes:
      chef_client:
        depends_client_rb: false
#+END_SRC

Including this makes it work (magic...).

Now it turns out I can't send monit a reload via SMF.  So I'll have to do that with an execute resource.

OK, I didnt correct the paths in the thin.yml, so needed to correct that, but now it doesn't restart:

#+BEGIN_SRC 
 ================================================================================
           Error executing action `restart` on resource 'service[monit]'
           ================================================================================
           
           Mixlib::ShellOut::CommandTimeout
           --------------------------------
           Command timed out after 600s:
       Command exceeded allowed execution time, process terminated
           ---- Begin output of /usr/sbin/svcadm disable -s monit ----
         * smartos_package[apache] action install  
           STDOUT: 
           STDERR: 
           ---- End output of /usr/sbin/svcadm disable -s monit ----
           Ran /usr/sbin/svcadm disable -s monit returned 
           
           Resource Declaration:
           ---------------------
           # In /tmp/kitchen/cache/cookbooks/base/recipes/_smartos.rb
           
         8: service 'monit' do
         9:   action [:enable]
        10: end
        11: 
           
           Compiled Resource:
           ------------------
           # Declared in /tmp/kitchen/cache/cookbooks/base/recipes/_smartos.rb:8:in `from_file'
       
       service("monit") do
             action [:enable]
             supports {:restart=>false, :reload=>false, :status=>false}
             retries 0
             retry_delay 2
             default_guard_interpreter :default
             service_name "monit"
             enabled true
             running true
             pattern "monit"
             declared_type :service
             cookbook_name "base"
             recipe_name "_smartos"
       end
           
       Recipe: wiki::default
         * execute[Reload Monit] action run
           - execute monit reload
       [2015-03-22T19:22:55+00:00] ERROR: Converge failed with error message service[monit] (base::_smartos line 8) had an error: Mixlib::ShellOut::CommandTimeout: Command timed out after 600s:
       Command exceeded allowed execution time, process terminated
       ---- Begin output of /usr/sbin/svcadm disable -s monit ----
       STDOUT: 
       STDERR: 
       ---- End output of /usr/sbin/svcadm disable -s monit ----
       Ran /usr/sbin/svcadm disable -s monit returned 

#+END_SRC

I suspect the stop method is insufficient. Hmm unsure.

Logged onto machine, ws able to stop and start fine.  However, noticed
that the config file still referred to conf.d not monit.d.  Fixed
that.  Now monit starts, and attempts to supervise thin, but without
joy.  I think the issue here is that the path is wrong:

#+BEGIN_SRC 
# ls -l /opt/local/wiki
total 48
-rwxr-xr-x  1 root root  59 Mar 22 18:21 git-wrapper
-rw-r--r--  1 wiki wiki 266 Mar 22 18:21 thin.yml
-rw-r--r--  1 wiki wiki 119 Mar 22 18:21 users.yml
drwxr-xr-x 12 wiki root  27 Mar 23 00:07 wiki
#+END_SRC



** Call with David and Luke
Had a call with David and Luke, and talked through the spreadsheet and
cyrus one.  The overall feeling was that we like the 60 drive machine
best, and should get a few of those, and 20 or so of the compute
nodes.  The main questions were:

- should we get them from the US directly, or via UK
- do we really need 10 -> 20 racks?  Could we settle on something smaller?
- what are the power whips? how does that impact?
- are the cross connects the ones between datacentres?
- are we fine to wire between racks?
- do we need to provide the racks?

We also need prices on the 10 dwpd disks (which look very expensive)
** Chat with Heinz re: MDC
Heinz hasn't played with MDC yet, but seems to think it will offer one of:

- Copies = N+(dc replica * dcs)
- N = LocalReplicas + (dc replicas * dcs)

He suggests testing, viz:

- if you have 6 free servers best thing would be to try it out
- set up to two Instances configure them as two DC's and see how the config affects it
- if you put 1 T into the one cluster you should see 3T disk useage there
- and 1T disk usage on the replica cluster

So if the idea is that we use 3N in the main site and 1N in DR, we still need 3 machines (or whatever).

I guess that makes the DR site 1/3, or a quarter overall.
* Mon 23 Mar 12:12:24 GMT 2015
** Sync with Nick
Sync'd with Nick on wiki stuff.  He had unpushed code to do with
Apache config, and when he pulled my changes had a bunch of stuff he
needed to stash or commit.

We looked into how to use ~git stash~ and ~git stash apply~ to stash
and then generate a merge, which worked well.

Nick has a lot of nasty sed in his cookbook, which we'll need to
address, but it's a start.

I started to show him my the ~.kitchen.yml~ changes, but we were
seeing odd behaviour where the machines were asking for passwords.
Simultaneously, SB and SJH complained that the HN seemed to be
unavailable.
** Broken Headnode
Asked NT to raise a ticket with Joyent.  The machine was unresponsive
to ssh, and when we consoled on, was still unresponsive.

Nick tracked the incident:

- https://gist.github.com/vxnick/2689992a98b126784aef
- https://gist.github.com/vxnick/c91b0af65b686644911f

Looked like a chassis issue.  We swapped the chassis out, and with
Joyent's help, we changed the mac, and the new system is up.

Joyent are doing some analysis to see what actually went wrong.
** Meraki Call
** Sentral Call
** Hammer Procurement
*** Switches
- ~ 7
- 
** Fahy Photo
** Cyrus One Call
*** External BW
- 400 commit
*** Space
- 10 - 12 - 14
- 10 static
- 15 static
*** Power
- Happy with power
- whips --> 1 time power within cost; non-recurring
- spec what we need; ask for recommendation.
*** Cross Connect
- Term from external
- IX cross nic
*** 2 Facilities
- Carrolton
- Houston... 
-- talk to Chris re: .. size/speed
- 6/7 primary
- 10 ms
- 10G quote prepared -- 
- 1G to follow
- maybe 10G/1G backup
- 1g -> 10g 60 days upgrade time
- 100% completely diverse
- alphius
- level 3

|-----------------+-------------+-------------+-------------|
|                 | DR1 (4-5-6) | DR2 (4 x 3) | DR3 (5 x 3) |
|-----------------+-------------+-------------+-------------|
| PRD1 (10 x 3)   |             |             |             |
| PRD2 (10-12-14) |             |             |             |
| PRD3 (15 x 3)   |             |             |             |
|-----------------+-------------+-------------+-------------|


** USA SMIC
*** Compute Spec:
- 2U 24 x 2.5" 920W (CSE-216BA-R920LPB)
- SM X10DRi
- 2 x E502690v3
- 256GB DDR4
- 2 x LSI 9207-8i HBA
- 1 x X520-DA2
- Rear 2 x 2.5" HDD kit (MCP-220-82609-0N)
*** JBOD Spec
- SM 4U 45* HSBay Red 1280w PSU + Rails (CSE-847E16-R1K28JBOD)
*** 1U Storage Head
- 1u SM 10* 2.5in HSBay Red 700w PSU + Rails (CSE-116TQ-R706WB)
- SM X10DRW-i
- Dual Intel E5-2603v2 (6*1.8Ghz/15MB/6.4GT's)
- 256GB DDR4 2133 ECC REG (16*16GB)
- LSI SAS 9207-8e (2* External Mini-SAS SFF8088)
- Intel X540-T2 (Dual Port 10GbE RJ45)
- 1 * SNK-P0047PSC & 1* SNK-P0057PS
- Riser for 2* Expansion (RSC-R1UW-2E16)
** Monit Work
Handy link: 
- http://hadzimahmutovic.com/monit-naemon/using-nagiosnaemon-monitoring-plugins-monit
* Tue 24 Mar 10:05:18 GMT 2015
** Adding FiFo Hypervisors
#+BEGIN_SRC 
VERSION=rel
cd /opt
curl -O http://release.project-fifo.net/chunter/${VERSION}/chunter-latest.gz
gunzip chunter-latest.gz
sh chunter-latest
svcadm enable epmd
svcadm enable chunter
#+END_SRC

#+BEGIN_SRC 
[root@0c-c4-7a-0f-aa-9a /opt]# dladm show-link 
LINK        CLASS     MTU    STATE    BRIDGE     OVER
e1000g0     phys      1500   down     --         --
e1000g1     phys      1500   up       --         --
ixgbe0      phys      1500   down     --         --
ixgbe1      phys      1500   up       --         --
[root@0c-c4-7a-0f-aa-9a /opt]# dladm show-phys -m
LINK         SLOT     ADDRESS            INUSE CLIENT
e1000g0      primary  c:c4:7a:f:aa:9b    yes  e1000g0
e1000g1      primary  c:c4:7a:f:aa:9a    yes  e1000g1
ixgbe0       primary  0:1b:21:9a:d0:38   yes  ixgbe0
ixgbe1       primary  0:1b:21:9a:d0:39   yes  ixgbe1
#+END_SRC

Add ~external_nic=0:1b:21:9a:d0:39~ to ~/usbkey/config~

Run ~sysinfo -u~

#+BEGIN_SRC 
[root@0c-c4-7a-0f-aa-9a /opt]# nictagadm list
NAME           MACADDRESS         LINK           TYPE            
external       00:1b:21:9a:d0:39  ixgbe1         normal          
admin          0c:c4:7a:0f:aa:9a  e1000g1        normal
#+END_SRC

** Make LeoFS N=3

On fifo and leo zones:

#+BEGIN_SRC 
svcadm disable leofs/manager
svcadm disable leofs/storage
svcadm disable leofs/gateway
rm -rf /var/db/leo*
#+END_SRC

Edit ~/opt/local/leo_manager/etc/leo_manager.conf~ and set:

#+BEGIN_SRC 
fifo> grep consistency /opt/local/leo_manager/etc/leo_manager.conf
##     * See: http://www.leofs.org/docs/configuration.html#the-consistency-level
consistency.num_of_replicas = 3
consistency.write = 2
consistency.read = 2
consistency.delete = 1
consistency.rack_aware_replicas = 0
fifo> 
#+END_SRC

Now start managers.

Aha

#+BEGIN_SRC 
11:06 <Cope> leo> leofs-adm status
11:06 <Cope> [ERROR] Mnesia is not available
#+END_SRC

Aha because I deleted all the things:

#+BEGIN_SRC 
USER=leofs
GROUP=$USER
COMPONENT=leo_manager
mkdir -p /var/db/leofs
chown -R $USER:$GROUP /var/db/leofs
mkdir -p /var/db/$COMPONENT/mnesia
mkdir -p /var/db/$COMPONENT/work
mkdir -p /var/db/$COMPONENT/snmp
chown -R $USER:$GROUP /var/db/$COMPONENT
mkdir -p /var/log/$COMPONENT/sasl
chown -R $USER:$GROUP /var/log/$COMPONENT 
#+END_SRC

#+BEGIN_SRC 
leo> leofs-adm status
 [System Confiuration]
---------------------------------+----------
 Item                            | Value    
---------------------------------+----------
 Basic/Consistency level
---------------------------------+----------
                  system version | 1.2.7
                      cluster Id | leofs_1
                           DC Id | dc_1
                  Total replicas | 3
        number of successes of R | 2
        number of successes of W | 2
        number of successes of D | 1
 number of DC-awareness replicas | 0
                       ring size | 2^128
---------------------------------+----------
 Multi DC replication settings
---------------------------------+----------
      max number of joinable DCs | 2
         number of replicas a DC | 1
---------------------------------+----------
 Manager RING hash
---------------------------------+----------
               current ring-hash | 
              previous ring-hash | 
---------------------------------+----------
#+END_SRC


OK so to create a zone.

First get the images:

#+BEGIN_SRC 
# imgadm update
Added manifest info for image 5a4ba06a-c1bb-11e4-af0b-4be0ce4ce04c from "https://images.joyent.com"
imgadm: warn: cannot determine original snapshot for image "5a4ba06a-c1bb-11e4-af0b-4be0ce4ce04c" (source info has no "dataset_guid")
# imgadm import d34c301e-10c3-11e4-9b79-5f67ca448df0

[root@0c-c4-7a-0f-aa-9a /opt]# vmadm list
UUID                                  TYPE  RAM      STATE             ALIAS
ff295131-b01a-47f0-8e25-e491a680e835  OS    1024     running           reginald
625b9489-7743-4efd-95a8-bb2c8af04ad3  OS    3072     running           leo-storage-2
#+END_SRC

Next we need a config.json:

#+BEGIN_SRC
cd /opt
cat <<EOF > leo-storage-3
{
 "autoboot": true,
 "brand": "joyent",
 "image_uuid": "d34c301e-10c3-11e4-9b79-5f67ca448df0",
 "max_physical_memory": 3072,
 "cpu_cap": 100,
 "alias": "leo-storage-3",
 "quota": "40",
 "resolvers": [
  "8.8.8.8",
  "8.8.4.4"
 ],
 "nics": [
  {
   "interface": "net0",
   "nic_tag": "admin",
   "ip": "10.130.254.32",
   "gateway": "10.130.254.254",
   "netmask": "255.255.255.0"
  }
 ]
}
EOF
#+END_SRC

Now make the image:

#+BEGIN_SRC 
[root@0c-c4-7a-0f-ab-22 /opt]# vmadm create -f leo-storage-3 

#+END_SRC



# vmadm list
UUID                                  TYPE  RAM      STATE             ALIAS
ff295131-b01a-47f0-8e25-e491a680e835  OS    1024     running           reginald
625b9489-7743-4efd-95a8-bb2c8af04ad3  OS    3072     running           leo-storage-2
# zlogin 625b9489-7743-4efd-95a8-bb2c8af04ad3

echo "http://release.project-fifo.net/pkg/rel/" >>/opt/local/etc/pkgin/repositories.conf
pkgin install leo_manager leo_storage leo_gateway

Edit ~/opt/local/leo_storage/etc/leo_storage.conf~

#+BEGIN_SRC 
managers = [manager_0@10.130.254.12, manager_1@10.130.254.11]
#+END_SRC

#+BEGIN_SRC 
svcadm enable epmd
svcadm enable leofs/storage
#+END_SRC


leo> leofs-adm start
Generating RING...
Generated RING
OK  33% - storage_0@10.130.254.12
OK  67% - storage_0@10.130.254.22
OK 100% - storage_0@10.130.254.32
OK
leo> leofs-adm status
 [System Confiuration]
---------------------------------+----------
 Item                            | Value    
---------------------------------+----------
 Basic/Consistency level
---------------------------------+----------
                  system version | 1.2.7
                      cluster Id | leofs_1
                           DC Id | dc_1
                  Total replicas | 3
        number of successes of R | 2
        number of successes of W | 2
        number of successes of D | 1
 number of DC-awareness replicas | 0
                       ring size | 2^128
---------------------------------+----------
 Multi DC replication settings
---------------------------------+----------
      max number of joinable DCs | 2
         number of replicas a DC | 1
---------------------------------+----------
 Manager RING hash
---------------------------------+----------
               current ring-hash | e402d7ab
              previous ring-hash | e402d7ab
---------------------------------+----------

 [State of Node(s)]
-------+------------------------------+--------------+----------------+----------------+----------------------------
 type  |             node             |    state     |  current ring  |   prev ring    |          updated at         
-------+------------------------------+--------------+----------------+----------------+----------------------------
  S    | storage_0@10.130.254.12      | running      | e402d7ab       | e402d7ab       | 2015-03-24 12:23:26 +0000
  S    | storage_0@10.130.254.22      | running      | e402d7ab       | e402d7ab       | 2015-03-24 12:23:27 +0000
  S    | storage_0@10.130.254.32      | running      | e402d7ab       | e402d7ab       | 2015-03-24 12:23:27 +0000
  G    | gateway_0@10.130.254.12      | running      | e402d7ab       | e402d7ab       | 2015-03-24 12:23:28 +0000
-------+------------------------------+--------------+----------------+----------------+----------------------------
** Creating a Fifo Cluster
NB cluster means 2 things... we don't mean cluster at the level of riak / leo.

#+BEGIN_SRC 
fifo> sniffle-admin vms init-cluster 75129287-0032-45f3-892a-9639cebd1574 FiFo
VM 75129287-0032-45f3-892a-9639cebd1574 added to new cluster ede671d0-8696-4eac-be04-83753fbf76fa.

#+END_SRC

*** Questions for Heinz
- Should we run the leo storage nodes on a different network?
- What is this: ~imgadm: warn: cannot determine original snapshot for image "5a4ba06a-c1bb-11e4-af0b-4be0ce4ce04c" (source info has no "dataset_guid")~
- Does sniffle-admin vms init-cluster take a list?
- Why is there no help for sniffle-admin vms init-cluster?
- How do I change the name of a vm?
** CyrusOne Call
* Wed 25 Mar 09:41:27 GMT 2015
** Email Review
- Novatech Conact in lieu of David: Chris Marrriot: 02392 322 537
- CPC Login:
#+BEGIN_EXAMPLE
username: davidyoungs
pass: dy2002lyc
#+END_EXAMPLE
- David Visa: 8165/459 (photo) 

** DNS Plan
** Wireless
* Thu 26 Mar 08:56:38 GMT 2015
** Mellanox Call with Chris from Hammer
2 devices proposed:

- 1036B
- 1012

Asked if they can be deployed in an 'end of rack' configuration, or in
a 'mesh'.  These don't have 'stacking ports', and are 100% optical.
He did indicate it might be possible to run copper with SFP
convertors, if that was a consideration.

The 1036 is a 36 port 40GB switch which be used for the 'spine', with TOR
1012s. The 1012 is a half-width switch with 40G ports which can be
broken into 4 10G switches using a hybrid QSFP -> 4 x SFP cable.

A consideration is that these can only run up to a maximum of 5M.  We
don't have a price for these cables yet.

The 1036 (and presumably the 1012) supports MLAG (which is good if we
want to avoid spanning tree).

The pricing which came through was $25000 for the two 1036 units, and
then utilizing a special bundle price for the 1012s - 5600 for a
switch and kit, with the option to add a second switch for a bit less.
In total on capex this looks like being about $61K.

I explained that we're comparing the Brocade 6970 and 6740.  The
former being a 40G switch, the latter a 10G.  We'd go for a 100%
copper solution with these.  Chris suggested setting up a call with a
tech-presales droid at Mellanox.  I agreed this was probably
worthwhile.  He also suggested he might be able to get us a demo model
(possibly to Texas).

With that in mind I've asked Brocade if they can do the same.

Follow up:

- Mellanox dude contact
- Demo Units
- Prices for hybrid cables

** 6 and 8TB disk tests
Let's have a look at 10.130.254.30:


$ ssh 10.130.254.30 -l root
Password: 
Last login: Tue Mar 24 12:16:06 2015 from 192.168.128.9
- SmartOS Live Image v0.147+ build: 20150306T202346Z

[root@0c-c4-7a-0f-ab-22 ~]# zpool create disktest mirror c0t50014EE260799002d0 c0t50014EE26098507Cd0 c0t50014EE2609631BBd0 c0t50014EE2607943CAd0

# zpool status zones
  pool: zones
 state: ONLINE
  scan: none requested
config: 

        NAME                     STATE     READ WRITE CKSUM
        zones                    ONLINE       0     0     0
          c0t50014EE20B0459ADd0  ONLINE       0     0     0

errors: No known data errors

[root@0c-c4-7a-0f-ab-22 ~]# zpool attach zones c0t50014EE20B0459ADd0 c0t50014EE20B5643A8d0




[root@0c-c4-7a-0f-ab-22 ~]# echo | format | grep c0t50014EE20B0459ADd0
      19. c0t50014EE20B0459ADd0 <ATA-WDC WD60EZRX-00M-0A80-5.46TB>

# zpool add -n zones mirrcor c0t50014EE2B5CF1A80d0 c0t50014EE2B5D0BE74d0
would update 'zones' to the following configuration:
        zones
          mirror
            c0t50014EE20B0459ADd0
            c0t50014EE20B5643A8d0
          mirror
            c0t50014EE2B5CF1A80d0
            c0t50014EE2B5D0BE74d0


# zpool status zones
  pool: zones
 state: ONLINE
  scan: resilvered 4.96G in 0h0m with 0 errors on Thu Mar 26 09:29:21 2015
config:

        NAME                       STATE     READ WRITE CKSUM
        zones                      ONLINE       0     0     0
          mirror-0                 ONLINE       0     0     0
            c0t50014EE20B0459ADd0  ONLINE       0     0     0
            c0t50014EE20B5643A8d0  ONLINE       0     0     0
          mirror-1                 ONLINE       0     0     0
            c0t50014EE2B5CF1A80d0  ONLINE       0     0     0
            c0t50014EE2B5D0BE74d0  ONLINE       0     0     0

errors: No known data errors


Keep in mind you're adding a new vdev which may have a different ashift than the already 
                existing mirror, so if you prefer to manually set an ashift you should use "zpool add -n 
                -o ashift=X zones mirror disk1 disk2", with X being 9 for 512 byte sectors or 12 for 4096


*** Tests to do:

Insert jpegs?
Resilver?
** DNS!!!
** Sentral Order
*** Firewall
*** Compute
- 2 x CN
- 60 x SSD
*** Storage
- 3 x AIC 60 drives (1 for havant)
- 3 x JBOD (1 for havant)
- 
* Fri 27 Mar 15:16:42 GMT 2015
** Brocade Loan
Carrollton
Carrollton

CyrusOne c/o LiveLink Technologies

1649 West Frankford Road

Carrollton, TX 75007

DH6
* Sun Mar 29 19:41:19 BST 2015
** Packing List
- [ ] Passport
- [ ] ESTA
- [ ] Printout of Destination
- [ ] Printout of Conference
- [ ] Clean Handkerchief
- [ ] Rollet
- [ ] Space Pen
- [ ] Screwdriver & Tool Singly (in big bag)
- [ ] Kindly
- [ ] Kindly Rire
- [ ] Telephone
- [ ] Telephone Rire
- [ ] Earphones
- [ ] Offline Mugate
- [ ] American Phone
- [ ] American Rire
- [ ] Study Books
  + [ ] Erlang
  + [ ] Performance
- [ ] Yellow Puter
- [ ] Yellow Puter Rire
- [ ] USA Plug Adaptors
- [ ] Apple Adaptor Kit
- [ ] Aeroplane Adaptor Kit
- [ ] Study Notebook
- [ ] Red pen
- [ ] Squishy ruler
- [ ] VGA, DVI, other singly convertors
- [ ] Travelly
- [ ] Yoga Mat
- [ ] Cushion
- [ ] Malas etc
- [ ] Dadramas
- [ ] Yoga shorts
- [ ] Yoga trousers
- [ ] Short-sleeved dragonfly top
- [ ] Long-sleeved dragonfly top
- [ ] Trainers
- [ ] Hoody
- [ ] Washbag
  + [ ] Toothpaste
  + [ ] Electric Toothbrush
  + [ ] Electric Toothbrush Rire
  + [ ] Razor
  + [ ] Undyarmdeodeant
  + [ ] Mensuns
    + [ ] Flowers
    + [ ] Echinacea
    + [ ] Bitman Mix
    + [ ] Bitamin B12
    + [ ] Headache Mensuns
- [ ] Spare Specs
- [ ] Lipsowve
- [ ] Handcream
- [ ] 3 shirts
- [ ] 3 pants
- [ ] 3 gocka
- [ ] Suit
- [ ] Shoes
