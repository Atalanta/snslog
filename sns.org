* Sun Mar 15 12:50:05 GMT 2015
** Things on my mind:

- Get the wiki up
- IRC cat
- Monitoring live
- Base on SmartOS
- ACL working
- S28/29 spreadsheet

Soas usual I am feeling like there's a big chain of dependencies.  I'd
like to do some stuff on SmartOS.  I'd like to be able to test it.  I
could do that manually by creating machines in JPC, and managing them
manually.  That would get things done.  It would be better to get that
plugged into TK though.

I'm going to go with primitive.

** Base SmartOS

This should be pretty straightforward.  First create a box.  Go to
JPC.  Click on create an instance, select base-64-lts, pick the size.

So next, what's the simplest, fastest way to get this machine
provisioned?  Again, it'd be great to have a knife bootstrap template.
But for now I'll do it manually.

OK so we have the IP:  72.2.113.95.  My Bosch key is good for LLJPC.

Now, for the purposes of an erb template, we need:

#+BEGIN_SRC 
pkgin -y in build-essential
pkgin -y in ruby
pkgin -y in ruby215-readline
gem install chef --no-rdoc --no-ri
#+END_SRC

I wonder if there is a better SmartOS omnibus package.  Seems not
really.  So we can look into this.

So now that chef is installed, we just need to keys and stuff.  I have
these on bosch.

Getting to bosch from here is easy enough because I configured
ssh... but which key?

#+BEGIN_SRC 
vermeer$ ssh-agent bash
vermeer$ ssh-add /home/sns/.ssh/id_dsa (Heidegger)
vermeer$ ssh-add /home/sns/.ssh/sns-laptop (Gentlemen)
vermeer$ ssh-add /home/sns/.ssh/bosch (None) **** THIS ONE
#+END_SRC

From Bosch:

#+BEGIN_SRC 
sns@bosch:/tmp$ rsync -Pvar chef root@72.2.113.95:/etc
#+END_SRC

And then on wikitest:

#+BEGIN_SRC 
# chef-client
DL is deprecated, please use Fiddle
[2015-03-15T13:38:42+00:00] INFO: Forking chef instance to converge...
Starting Chef Client, version 12.1.1
[2015-03-15T13:38:43+00:00] INFO: *** Chef 12.1.1 ***
[2015-03-15T13:38:43+00:00] INFO: Chef-client pid: 32168
Creating a new client identity for wikitest1 using the validator key.
[2015-03-15T13:38:46+00:00] INFO: Client key /etc/chef/client.pem is not present - registering
[2015-03-15T13:38:47+00:00] INFO: HTTP Request Returned 404 Object Not Found: error
[2015-03-15T13:38:48+00:00] INFO: Run List is []
[2015-03-15T13:38:48+00:00] INFO: Run List expands to []
[2015-03-15T13:38:48+00:00] INFO: Starting Chef Run for wikitest1
[2015-03-15T13:38:48+00:00] INFO: Running start handlers
[2015-03-15T13:38:48+00:00] INFO: Start handlers complete.
resolving cookbooks for run list: []
[2015-03-15T13:38:48+00:00] INFO: Loading cookbooks []
Synchronizing Cookbooks:
Compiling Cookbooks...
[2015-03-15T13:38:48+00:00] WARN: Node wikitest1 has an empty run list.
Converging 0 resources
[2015-03-15T13:38:48+00:00] INFO: Chef Run complete in 0.789150147 seconds

Running handlers:
[2015-03-15T13:38:48+00:00] INFO: Running report handlers
Running handlers complete
[2015-03-15T13:38:48+00:00] INFO: Report handlers complete
Chef Client finished, 0/0 resources updated in 5.862599973 seconds
[2015-03-15T13:38:48+00:00] INFO: Sending resource update report (run-id: ffc699d3-b25b-431a-80e1-d4a599c3c7d6)
#+END_SRC

OK... so we want to check out the base:

#+BEGIN_SRC
sns@bosch:~/src/livelink-chef-repo/roles$ cat base.rb 
name 'base'
description 'Shared role for all systems'
run_list 'recipe[base]'
#+END_SRC

Update the runlist:

#+BEGIN_SRC 
sns@bosch:~/src/livelink-chef-repo$ knife node run_list set wikitest1 'role[base]'
#+END_SRC


Running chef-client--- bugger:

#+BEGIN_SRC 
    * cannot determine group id for 'sbradly', does the group exist on this system?
#+END_SRC


OK, so, on SmartOS the user resource won't create a group.  Feels to
me like the acl cookbook is needed now.

** ACL Cookbook
#+BEGIN_SRC 
20:00 <Cope> I have a requirement to manage users on a fairly granular level, in terms of what each user can (and cannot) do, and over what machines.
20:01 <Cope> my idea is to have an acls object (currently from a databag) which has keys corresponding to hostnames or patterns to match hostnames, and a default
20:02 <Cope> with the values corresponding to a scale of permission level, where -1 is locked, 0 is no user, 1 is restricted user, 2 is login user, 3 is login user with some sudo power and 4 is full sudo
20:02 <Cope> so each user gets one of these objects
20:02 <Cope> and then on each node a decision can be made as to what sort of shell, what sort of sudo privs, etc should be in place
20:03 <Cope> i wanted to avoud a whole mess of conditional logic in recipe code
20:03 <Cope> so something that generates the data which defines the appropriate level of access for a user on a node
20:03 <Cope> which is then handed to regular resources
20:04 <Cope> this needs to work on disparate platforms too, so again, generating the data first, and working out what needs to be called could be done outside of teh recipe
20:05 <Cope> i notice that the cool kids don't lwrp any more... so that's a consideratiob
20:05 <Cope> anyway
20:05 <Cope> thats the outline idea... what do you think?
#+END_SRC

Looked at MySQL cookbook.  Seems like this is a good pattern.  Parking for now.
** LeoFS Call with Heinz Geis
Scale - few big servers
600TB in each (not ideal)
Would prefer more small servers
Works better this way
recovery is better
rep set to 3
Why fewer large?  because there before
may as well use them
3TB SATAs x 60

Packages in project fifo
Operationally: hard to monitor
Checks if machines are down
JSON calls to API
Behaves well
if it goes down (system crash, network failure) restart
been using since 0.6
No problems since 1.1
Entire network rewiring
Update procedure - a bit manual
Tell machine down, update and back up again
Cannot chaneg replication factor after
Need to decide now
3 storage zones - then add a 2nd... and
very easy to add
say where is the master
no masterless setup
ring like (dynamo)
2 management servers (client/master)
coordination here
rebalance suffles
1 gateway
zfs compression on storage servers
still benefit (over many images)
jpeg header always same... so there is some benefit
zfs is smart - won't copress
what goes wrong?
worst - compact?
manually tell it to delete old data
not perfect with s3 clients
large datasets - no problem with 
large data in chyunks
1GB zone image --> leofs chunk into 5MB chunks
smaller just stored as 1 object
can send zfs data into leofs
chunk reads & writes
uploads chunks in parallel
download is the same
tricky to sync r/w size
classical approach of streaming up and down, gts around problem
1-2MB no problem
s3cmd or dragondisk osx - classic read and write non-chunked
part of s3 api (so can ignore it... which would be fine)

LeoFS - multi data centre aware
async bkg replication between datacentres
very sensible & smart
keep 1 extra copy in each remote DC *(or more)
3 copies + 1 in remote
for DR... if we die, we have the data
and quicker remote read access
Japanese Amazon!
Share data
Perf baseline:

cat sat 10G ether
multi writes in parallel
reads: 5GB +/-
stripe of mirrors
stripe of raidz2
mirrors zfs log & cache
scalable informatics
1/2 TB memory
e5 3.4 procesors
512GB

2U many disks (20ish)
128+ memory
reads - memory... good
fifo contains monitoring
rest by zabbix
ZFS send is 100MB/s
concat witg gzip -1
sweet spot...
50GB....

data frequently accessed in l1 and l2 arc

most data not needed; can build who storage to be fast
can be as big as memory

data on disk - lowest
ssd act as l2 arc (not in memory... often used)
l 1 = zfs using spare memory

cache = 2 x ssds

4TB ssd cache
many disks

zpool

1GB memory & 100GB disk - lose performance
becasue where stuff is layed out on disk is in memory... is there a rule of thumb

l1 -> l2 -> arc cache

zil / slog? enough to written to logs

picked because R&W perf is much better
basho sales were dicks
* Wed 18 Mar 18:39:45 GMT 2015
** Pound
Relayd isn't working well, so we need a replacement.
Suggested/recommended Pound.

    pkg_add pound

Now, let's empty the config file.

#+BEGIN_SRC 
-bash-4.3# > /etc/pound.cfg
-bash-4.3# pound
starting...
no listeners defined - aborted
#+END_SRC

Great, so we need some listeners.  They can be HTTP or HTTPS.  Let's
start with an HTTP one.  Listeners need an address and a port.  The
address is just localhost, and the port can be 8080.

This is enough for pound to start:

#+BEGIN_SRC 
# telnet localhost 8080
Trying 127.0.0.1...
Connected to localhost.
Escape character is '^]'.
get / http/1.1

HTTP/1.0 503 Service Unavailable
Content-Type: text/html
Content-Length: 53
Expires: now
Pragma: no-cache
Cache-control: no-cache,no-store

The service is not available. Please try again later.Connection closed by foreign host.
#+END_SRC

Fairly obviously, pound doesn't know what to do with requests.  Let's
fix that.  For this we need a service.  Services can either be global
or tied to listeners.  For now let's make it global.

Now, services need backends.  Again, the simplest thing possible would
be to pass everything to one machine.

This works:

#+BEGIN_SRC 
-bash-4.3# telnet localhost 8080
Trying 127.0.0.1...
Connected to localhost.
Escape character is '^]'.
get / http/1.1

HTTP/1.1 400 Bad Request
Date: Wed, 18 Mar 2015 19:11:26 GMT
Server: Apache/2.4.7 (Ubuntu)
Content-Length: 320
Connection: close
Content-Type: text/html; charset=iso-8859-1

<!DOCTYPE HTML PUBLIC "-//IETF//DTD HTML 2.0//EN">
<html><head>
<title>400 Bad Request</title>
</head><body>
<h1>Bad Request</h1>
<p>Your browser sent a request that this server could not understand.<br />
</p>
<hr>
<address>Apache/2.4.7 (Ubuntu) Server at l0001.rs.uk.livelinkprint.com Port 80</address>
</body></html>
Connection closed by foreign host.
#+END_SRC


Now, let's make it a bit more complicated.

Let's have two services, one for costcophoto.co.uk, and one for
lift.livelinkprint.com.  OK that works... (be careful with spaces in
vhost names - the space needs to be there to match Host:
myhost.foo.com)

OK, next, can we have multiple matches per service, so we can pass in
for, eg, s28 and s29 domains?  No.  We need a service per host.

Right, this all seems to work.  The next challenge is an HTTPS listener.

So, for this we need to add a listener and cert.

The cert needs all three in one:

    -bash-4.3# cat livelinkprint.com.key livelinkprint.com.crt livelinkprint.com.crt2 > livelinkprint.com.pem

So with the HTTPS listener in place, everything works and is in Chef
and the service is running.  But we need it to be in chef.

Not all straightfoward in Chef as the package and service resources
don't quite do the right thing, so I've botched it using execute
resources.

OK, in Chef and pushed.
* Thu 19 Mar 09:12:20 GMT 2015
** SNS Log
Maintaining a log is a good idea.  But let's ensure it's in git, so I can get at it any time:

#+BEGIN_SRC
sns@bosch:~/doc$ cd sns/
sns@bosch:~/doc/sns$ ls
sns.org
sns@bosch:~/doc/sns$ git init
Initialized empty Git repository in /home/sns/doc/sns/.git/
sns@bosch:~/doc/sns$ git add sns.org 
sns@bosch:~/doc/sns$ git commit -m 'SNS Log'
[master (root-commit) 0611a8d] SNS Log
 1 file changed, 134 insertions(+)
 create mode 100644 sns.org
sns@bosch:~/doc/sns$ git remote add origin git@github.com:Atalanta/snslog.git
sns@bosch:~/doc/sns$ git push -u origin master
Counting objects: 3, done.
Delta compression using up to 4 threads.
Compressing objects: 100% (2/2), done.
Writing objects: 100% (3/3), 2.08 KiB | 0 bytes/s, done.
Total 3 (delta 0), reused 0 (delta 0)
To git@github.com:Atalanta/snslog.git
 * [new branch]      master -> master
Branch master set up to track remote branch master from origin.
#+END_SRC
** Emacs Learnings
- To prevent ido-mode from switching to a file/buffer that is already open (such as default.rb), use *ido-fix* (C-j)
- To create src or quote blocks quickly, in org-mode, type in "<s" or "<q" and press TAB
** Pound follow up
Pound has been installed on the main firewall:

#+BEGIN_SRC
Host rsbastion
  HostName 109.200.18.70
  User sns
  Port 3023
IdentityFile ~/.ssh/id_dsa
#+END_SRC

Chef has been updated accordingly, and relayd switched off.  The
firewall is now configured to redirect http and https traffic to
localhost, wherefrom Pound sends it to the right places:

#+BEGIN_SRC
pass in quick on $ext_if proto tcp from any to $ext_if port http rdr-to 127.0.0.1
pass in quick on $ext_if proto tcp from any to $ext_if port https rdr-to 127.0.0.1
#+END_SRC

Pound is running out of rc.d, but not monitored or supervised in
anyway.  It would be a good idea to have the process watched using
monit.

Logs presently are in pound format, but there's an option to make them
apache format which might be useful for analysis purposes.

The logs are probably not being rotated, and there is no other monitoring on the box.

- Created ticket:
  https://livelinkinfraops.zendesk.wcom/agent/tickets/130 for log
  rotation.

- Created ticket:
  https://livelinkinfraops.zendesk.com/agent/tickets/131 for
  monitoring.

Use of Relayd also isn't documented, which is largely a function of
the wiki not being available.

The process supervision is required for both pound and thin, so it's
important to get right.
** Wiki Unavailable
Discussed with NT & SB.  Current status is that the wiki exists in so
far as there's a git repo, but it is hard to use.  I'd already created
a prototype wiki at 37.153.107.245.

This used varnish to proxy to thin, and managed thin using monit:

#+BEGIN_SRC
[root@28e8a2a4-7db4-ea0c-afeb-be0b963334f0 ~]# cat /opt/local/etc/default.vcl 
backend default {
    .host = "127.0.0.1";
    .port = "3000";
}

sub vcl_fetch {
    if (beresp.status == 302 && beresp.http.location ~ "/create") { return (hit_for_pass); }  }
#+END_SRC

This is needed because when you create a site, there's a redirect, which gets cached.  Actually there are other problems with Varnish which are unresolved:

#+BEGIN_SRC
09:14 <Cope> Still have an oddity with my varnish / gollum setup... 
09:15 <Cope> http://37.153.107.245:8080/Home
09:15 <Cope> 1) Create a page & save
09:15 <Cope> 2) Edit the page and save
09:15 <Cope> When we visit the edit page we don't (always) see the latest version
09:15 <Cope> and if we do
09:16 <Cope> 3) When we edit it again, we see a cache of the last edit!
09:16 <Cope> turns out caching is hard.
09:37 <Mithrandir> either have it send out a purge on edit, or look at what it looks like when you edit, then purge the object for that url
09:41 <Cope> what's the best way to see what is actually happening?
09:41 <Mithrandir> I tend to just use the network viewer in chromium
09:41 <Mithrandir> but wireshark works too
#+END_SRC

To run with thin, we just need a config.ru in the base of the directory we want to use.  Nick has augmented this to implement some authentication:

#+BEGIN_SRC 
__DIR__ = File.expand_path(File.dirname(__FILE__))
$: << __DIR__
require 'rubygems'
require 'yaml'
require 'app'
App.set(:gollum_path, __DIR__)
App.set(:authorized_users, YAML.load_file(File.expand_path('users.yml', __DIR__)))
App.set(:wiki_options, {})
run App 
#+END_SRC

This requires that we have an app.rb and a users.yml file too:

#+BEGIN_SRC 
require 'gollum/app'
require 'digest/sha1'

class App < Precious::App
  User = Struct.new(:name, :email, :password_hash, :can_write)

  before { authenticate! }
  before /^\/(edit|create|delete|livepreview|revert)/ do authorize_write! ; end

  helpers do
    def authenticate!
      @_auth ||=  Rack::Auth::Basic::Request.new(request.env)
      if @_auth.provided?
      end
      if @_auth.provided? && @_auth.basic? && @_auth.credentials &&
        @user = detected_user(@_auth.credentials)
        return @user
      else
        response['WWW-Authenticate'] = %(Basic realm="Gollum Wiki")
        throw(:halt, [401, "Not authorized\n"])
      end
    end

    def authorize_write!
      throw(:halt, [403, "Forbidden\n"]) unless @user.can_write
    end

    def users
      @_users ||= settings.authorized_users.map {|u| User.new(*u) }
    end

    def detected_user(credentials)
      users.detect do |u|
        [u.email, u.password_hash] ==
        [credentials[0], Digest::SHA1.hexdigest(credentials[1])]
      end
    end
  end

  def commit_message
    {
      :message => params[:message],
      :name => @user.name,
      :email => @user.email
    }
  end
end
#+END_SRC

The users.yml looks like this:

#+BEGIN_SRC 
---
- - Nick Trew
  - n.trew@livelinktechnology.net
  - e5e9fa1ba31ecd1ae84f75caaa474f3a663f05f4 # secret
  - true 
#+END_SRC

So NT & SB and I talked about the hosting environment for the wiki.  I
suggested continuing with thin, but with apache and mod_proxy_balancer
in place of passenger.  SB pointed out that we do wish to keep things
standard, but was open to trying an alternative, as long as we agreed
to standardise.

The requirement for authentication was also discussed.  I suggested
using Basic Auth with Apache, but NT managed to find a way to do it in
the application.  Either way, it works fine.  We just need to ensure
we run it via https (using the wildcard certificate).

We agreed that we'd like to run this in RedStation.

Next steps:

- Finish the monit supervision to run with multiple thin processes
- Write an SMF manifest to run monit
- Get Chef to drop off the user.yml

Nick's making a wiki cookbook.
** Chef Environment Issue
Yesterday I made breaking changes to the firewall cookbook (replacing
relayd with pound), so bumped the version to 1.0.0.  Using knife
spork, I did cookbook version bumping and promoting, which worked
fine.  The state being that when I had finished, the testing
environment specified 1.0.5, the firewall was using the testing
environment, and the newest version (with the pound changes) was
uploaded.

This morning, SJH made changes to correct the egress and ingress rules
for UDP and TCP, and duly bumped the version and environment.
However, because I failed to push my environment to git, Stuart's
local copy was from a time when the version was 0.12ish, so he ended
up setting the environment to a lower version.  This could have had
the effect of pushing out changes from an earlier version.

Lesson: ensure you push your environments!

Thoughts from Jon:

#+BEGIN_SRC 
13:35 <Cope> https://gist.github.com/Atalanta/5b067ad755a2e79acd6d
13:36 <jonlives> the short version for how we catch that is https://github.com/bmarini/knife-inspect, run as a jenkins job on every git push
13:41 <Cope> so jenkins watches git
13:41 <Cope> and runs knife-inspect
13:41 <Cope> and bitches out
13:42 <jonlives> if anything is out of sync
13:42 <jonlives> which includes constraints etc
13:42 <Cope> so it won't stop the push, but it will warn us that it is wrong
13:42 <jonlives> yeah
#+END_SRC
** Monit Process Supervision
#+BEGIN_SRC 
[root@28e8a2a4-7db4-ea0c-afeb-be0b963334f0 /opt/local/etc/monit]# grep -v ^# monitrc 
set daemon  60              # check services at 1-minute intervals
set mailserver localhost
set alert sanelson@gmail.com not on { instance, action }
set httpd port 2812 and
    allow admin:monit      # require user 'admin' with password 'monit'
    allow @monit           # allow users of group 'monit' to connect (rw)
    allow @users readonly  # allow users of group 'users' to connect readonly


check process thin with pidfile /root/mithrandir/tmp/pids/thin.pid
    start program = "/opt/local/bin/thin -d -c /root/mithrandir start"
    stop program = "/opt/local/bin/thin -c /root/mithrandir stop"
#+END_SRC

So monit needs to know:

- what the process should be called
- what the pid is
- what the start program is
- what the stop program is

For thin, we'd want to have separate processes for each one, rather
than start a cluster, as deriving the pids would be trickier.

** Push after or before a promote --remote
This is a discussion - we need to do one or the other.  I know Etsy
have struggled with this.  They settled for after, because the server
is the source of truth for the nodes, and git for the humans.

** Test Kitchen for OpenBSD and SmartOS
NT has implemented some of this... need to find out how much.
Specifically I am thinking of the fact that the firewall is using the
testing environment, but that the testing environment lags behind the
latest version.
** FahyFoto R3 Site
https://livelinkinfraops.zendesk.com/agent/tickets/124
Not sure what the process is here, or what the deadline is.

Damon says there's no real deadline, but that things should really
speed up a bit.  The dude has been waiting for 2 weeks.  I'm not sure
what the status us, but I've said I'll look into it.  Also I've
suggested that we have a chat with Ed and map out the flow and see
what we can do to speed it up.



** Walmart Hardware Order
Yesterday I had a long session with Guy in which we outlined our
thoughts about size and spec of machines.  I need to convert this into
a spreadsheet and write it up for David today.  In principle, I think
we are of the view that 60 drives machines could be very effective.
To that end I asked Sentral to match the basic spec that Doug and
Robert at Hammer had come up with.  They've done so, and come in at
about the price I expected.

I also had a very long conversation with the founder/ceo of 'Scalable
Informatics', whom Heinz recommended.  He also knows Phil Hollenback.
They build 60 drives machines to a high spec, and layout the ZFS stuff
and offer a 3 year support package.  This seems excellent but looks
like coming in at about 2 or 3 times more expensive per box, which at
11 boxes is not insignificant.  I think perhaps we might want to try
out one of them?
** Provisioning a Ruby Site
Spoke with SJH & SB about the steps to set up a Ruby site, such that
it's ready to be deployed with Capistrano.  SJH outlined the following:

- Provision Apache & Passenger
- Create a deploy user
- Add a vhost
- Create an SMF manifest for apache with secret.key.base
- Create a DB (if it doesn't exist)
- Grant the privileges / create the user
- Render the shared/database.yml
- Create an internal DNS record for the zone
- Create an external DNS record for the zone
- Add appropriate entries to the pound config

I gave a history lesson about starting from raw resources, then using
definitions, then LWRPs then resources in a library, and recommended
they start with the newest approach.  SJH asked about orchestration,
and pointed out that Chef is very much focussed on a node by node
basis, but that he is thinking at a higher level - how would we do this is Chef?

I mentioned Chef Provisioning as something to look at, but also felt
that for much of the stuff simply using Chef search ought to be
sufficient.
** CyrusOne Call
Spoke with Colleen this morning (her time).  She's working to get the
paperwork ready into a signable bundle by tomorrow morning her time.
From our side we simply need to decide if we're happy with the MSA and
standard T&C.  I think we are, but I need to check with David.

On my side, we need to agree the size of network we need, so we can
justify IP addresses.

Touch call again tomorrow at the same time.
** Sentral Storage Discussion
Had a call with Mike & Mike.  They say that they cannot get the 60
drive machine until late April, which is less than ideal.  They
suggest a 1U machine with the same spec and one or two 4U JBODs.
These have:

- 45 3.5 drives per chassis in a 4U enclosure
- 4 ports out the back

They also have one which takes 90, but this is 2 drives per hotswap,
which would require some zpool thinking.

The Mikes are going to get back to me with pricing ASAP.
** Ganglia
#+BEGIN_SRC 
marking ganglia-webfrontend-3.1.2nb2 as non auto-removable
[root@737e21f2-8f99-e8f9-ff4e-8a530796050e ~]# lynx http://localhost/ganglia/
[root@737e21f2-8f99-e8f9-ff4e-8a530796050e ~]# ifconfig 
lo0: flags=2001000849<UP,LOOPBACK,RUNNING,MULTICAST,IPv4,VIRTUAL> mtu 8232 index 1
        inet 127.0.0.1 netmask ff000000 
net0: flags=40201000843<UP,BROADCAST,RUNNING,MULTICAST,IPv4,CoS,L3PROTECT> mtu 1500 index 2
        inet 10.128.2.6 netmask ffffff00 broadcast 10.128.2.255
        ether 90:b8:d0:e2:ee:e2 
lo0: flags=2002000849<UP,LOOPBACK,RUNNING,MULTICAST,IPv6,VIRTUAL> mtu 8252 index 1
        inet6 ::1/128 
[root@737e21f2-8f99-e8f9-ff4e-8a530796050e ~]# man svccfg
[root@737e21f2-8f99-e8f9-ff4e-8a530796050e ~]# logout
Connection to 10.128.2.6 closed.
#+END_SRC
* Sat Mar 21 20:31:48 UTC 2015
** Workstation
I managed to fill up / on my OpenBSD workstation, so I needed a
rebuild.  Decided to give FreeBSD a go.  Downloaded the image and
discovered that on the mac it is very slow to dd, resulting in me
being impatient, and pulling out the stick before it was finished.  To
show progress as dd runs, we can use pv:

#+BEGIN_SRC 
pv -ptearb FreeBSD-10.1-RELEASE-amd64-mini-memstick.img | dd of=/dev/rdisk2 bs=1024
#+END_SRC

I installed FreeBSD and then went into /usr/ports/x11/xorg and did a make install clean.

I forgot to do a make config-recursive, but remembered next time!

Next up: i3lock, i3status and i3.

Other things I will want:

- screen
- emacs
- firefox
- ruby
- dmenu
- rxvt-unicode
- feh?
- inconsolata font

Once all installed, I rsync'd back from Chenrezig.  This is all a bit
messy, and needs to be tidied up, but I have all teh data, and the
Firefox cache even came over!

One consideration is that FreeBSD does not read .Xresources by
default. And when I copied it over to .Xdefaults, it came up in pink!

However, xrdb ~/.Xresources worked fine, so I shall add that to my .xinitrc, which currently says:

#+BEGIN_SRC 
[[ -f ~/.Xresources ]] && xrdb -merge ~/.Xresources
setxkbmap -layout gb -option ctrl:nocaps &
xsetroot -solid black &
exec i3
#+END_SRC

So I think if I just remove the -merge bit, it should be fine.  Let's test that out.
